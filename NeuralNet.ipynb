{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "nb_discussion = 3\n",
    "nb_replique_max = 5\n",
    "\n",
    "#TEXT\n",
    "text_decoupe = [[\"The sun is shining in June! It is beautiful.\",\"September is grey.\",\"Life is beautiful in August.\",\n",
    "         \"I like it\",\"This and other things?\"],\n",
    "        [\"The sun is shining in Fevrier! It is beautiful.\",\"Decembre is grey.\",\"Life is beautiful in Monday.\",\n",
    "         \"I like it\",\"This and other things?\"],\n",
    "        [\"The sun is shining in Tuesday! It is beautiful.\",\"Juin is grey.\",\"Life is beautiful in August.\",\n",
    "         \"I like it\",\"This and other things?\"]]\n",
    "text = [\"The sun is shining in June! It is beautiful.\",\"September is grey.\",\"Life is beautiful in August.\",\n",
    "         \"I like it\",\"This and other things?\", \"The sun is shining in Fevrier! It is beautiful.\",\"Decembre is grey.\",\n",
    "         \"Life is beautiful in Monday.\",\"I like it\",\"This and other things?\",\n",
    "         \"The sun is shining in Tuesday! It is beautiful.\",\"Juin is grey.\",\"Life is beautiful in August.\",\n",
    "         \"I like it\",\"This and other things?\"]\n",
    "\n",
    "\n",
    "#TOKENIZER\n",
    "token = keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                           lower=True, split=\" \", char_level=False)\n",
    "token.fit_on_texts(text)\n",
    "nombre_mots = len(token.word_counts)\n",
    "\n",
    "data = np.zeros((nb_discussion, nb_replique_max, nombre_mots))\n",
    "\n",
    "for i in range(0,nb_discussion):\n",
    "    data[i,:,:] = token.texts_to_matrix(text_decoupe[i])[:,1:]\n",
    "    \n",
    "    \n",
    "#EMBEDDING\n",
    "embedding_output_dim = 10\n",
    "\n",
    "input_text = keras.layers.Input(shape =(nb_replique_max,nombre_mots,), name = \"main\")\n",
    "embedding_layer = keras.layers.Embedding(nombre_mots, embedding_output_dim, input_length = None,\n",
    "                                        embeddings_initializer='uniform',embeddings_regularizer=None, \n",
    "                                        activity_regularizer=None,embeddings_constraint=None,\n",
    "                                        mask_zero=False)(input_text)\n",
    "\n",
    "\n",
    "#BAG_OF_WORDS\n",
    "#bag = keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "bag = keras.layers.Lambda(lambda x: K.sum(x, axis=2), output_shape=(nb_replique_max, embedding_output_dim,))(embedding_layer)\n",
    "\n",
    "\n",
    "#INPUTBIS\n",
    "A = np.random.rand(3,5,4)\n",
    "\n",
    "embedding_incise_dim = 4\n",
    "input_incise = keras.layers.Input((nb_replique_max, embedding_incise_dim, ), name = \"aux\")\n",
    "\n",
    "\n",
    "#MERGING\n",
    "merged = keras.layers.concatenate([bag, input_incise])\n",
    "\n",
    "\n",
    "#LSTM\n",
    "units = 6\n",
    "lstm = keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                  kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                  unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,\n",
    "                  activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "                  dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False,\n",
    "                  go_backwards=False, stateful=False, unroll=False) (merged)\n",
    "\n",
    "\n",
    "#MODEL\n",
    "model = keras.Model(inputs = [input_text, input_incise] , outputs = lstm)\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#print(model.predict([data,A]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONEHOT_ENCODING\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "entree = [[2,1,2,1,2],[1,2,3,4,5],[3,6,5,0,0]]\n",
    "\n",
    "nb_personnages = np.max(entree)\n",
    "enc = OneHotEncoder(n_values = nb_personnages + 1)\n",
    "\n",
    "enc.fit(entree)\n",
    "Y = enc.transform(entree).toarray().reshape(nb_discussion, nb_replique_max, nb_personnages + 1)[:,:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3/3 [==============================] - 1s 308ms/step - loss: 2.8737 - acc: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5256da390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data,A], Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donne = []\n",
    "total = []\n",
    "discussion = []\n",
    "personnage = []\n",
    "personnage_total = []\n",
    "discussion_int = 1\n",
    "dictionnaire = dict()\n",
    "dic_int = 1\n",
    "\n",
    "with open(\"corpus/REAL_ALL_CONTENTS_PP.txt\",\"r\") as f:\n",
    "    text = f\n",
    "    for line in text:\n",
    "        block = line.split(\"\\t\")\n",
    "        if(int(block[0]) != discussion_int):\n",
    "            discussion_int += 1\n",
    "            donne.append(discussion)\n",
    "            discussion = []\n",
    "            personnage_total.append(personnage)\n",
    "            personnage = []\n",
    "        if(block[1] not in dictionnaire):\n",
    "            dictionnaire[block[1]] = dic_int\n",
    "            dic_int += 1\n",
    "        personnage.append(dictionnaire.get(block[1]))\n",
    "\n",
    "        discussion.append(block[2])\n",
    "        total.append(block[2])\n",
    "    donne.append(discussion)\n",
    "    personnage_total.append(personnage)\n",
    "l_max = max([len(disc) for disc in donne])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = keras.preprocessing.text.Tokenizer(num_words = 100, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                           lower=True, split=\" \", char_level=False)\n",
    "token.fit_on_texts(total)\n",
    "#longueur = len(token.word_counts)\n",
    "longueur = 100\n",
    "\n",
    "data = np.zeros((discussion_int, l_max, longueur))\n",
    "\n",
    "for i in range(0,discussion_int):\n",
    "    temp = token.texts_to_matrix(donne[i])#[:,1:]\n",
    "    if(l_max != len(temp)):\n",
    "        pad = np.zeros((l_max-len(temp),longueur))\n",
    "        Mat = np.concatenate((temp,pad), axis = 0)\n",
    "    data[i,:,:] = Mat\n",
    "people = []\n",
    "\n",
    "for pers in personnage_total:\n",
    "    people.append(np.pad(pers, (0,l_max-len(pers)), 'constant', constant_values=(0, 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_personnages = np.max(people)\n",
    "enc = OneHotEncoder(n_values = nb_personnages + 1)\n",
    "\n",
    "enc.fit(people)\n",
    "Y = enc.transform(people).toarray().reshape(discussion_int, l_max, nb_personnages + 1)#[:,:,1:]\n",
    "\n",
    "embedding_incise_dim = nb_personnages + 1\n",
    "Z = np.zeros((discussion_int, l_max, embedding_incise_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "li = []\n",
    "data_zero = np.zeros((discussion_int, l_max, longueur))\n",
    "# res = model.predict([data_zero[0:1], Z[0:1]])\n",
    "# for i in range(0,l_max):\n",
    "#     li.append(np.argmax(res[0,i,:]))\n",
    "# print(li)\n",
    "\n",
    "#FAUX_EXEMPLE\n",
    "from random import randint\n",
    "Y_fake = np.zeros((discussion_int, l_max, nb_personnages+1))\n",
    "Z_fake = np.zeros((discussion_int, l_max, nb_personnages+1))\n",
    "\n",
    "for i in range(discussion_int):\n",
    "    for j in range(l_max):\n",
    "        r = randint(0,nb_personnages)\n",
    "        Y_fake[i,j,r] = 1\n",
    "        Z_fake[i,j,r] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "61/61 [==============================] - 6s 96ms/step - loss: 3.5098 - acc: 0.1242\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.4777 - acc: 0.1159\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.4506 - acc: 0.1159\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.4282 - acc: 0.1159\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.4091 - acc: 0.1159\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.3924 - acc: 0.1159\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.3773 - acc: 0.1159\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.3633 - acc: 0.1159\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 2s 30ms/step - loss: 3.3501 - acc: 0.1159\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 2s 31ms/step - loss: 3.3373 - acc: 0.1159\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 3.3250 - acc: 0.1159\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 3.3129 - acc: 0.1159\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 3.3009 - acc: 0.1159\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 2s 32ms/step - loss: 3.2891 - acc: 0.1159\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 2s 33ms/step - loss: 3.2772 - acc: 0.1159: 0s - loss: 3.2818 - \n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 2s 31ms/step - loss: 3.2654 - acc: 0.1159\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 2s 31ms/step - loss: 3.2534 - acc: 0.1159\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 2s 31ms/step - loss: 3.2414 - acc: 0.1159\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - 2s 31ms/step - loss: 3.2292 - acc: 0.1159\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 2s 29ms/step - loss: 3.2169 - acc: 0.1159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5b1827198>"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_discussion = discussion_int\n",
    "nb_replique_max = l_max\n",
    "nombre_mots = longueur\n",
    "embedding_incise_dim = nb_personnages+1\n",
    "\n",
    "#EMBEDDING\n",
    "embedding_output_dim = 1\n",
    "\n",
    "input_text = keras.layers.Input(shape =(nb_replique_max, nombre_mots,), name = \"main\")\n",
    "embedding_layer = keras.layers.Embedding(nombre_mots, embedding_output_dim, input_length = None,\n",
    "                                        embeddings_initializer='uniform',embeddings_regularizer=None, \n",
    "                                        activity_regularizer=None,embeddings_constraint=None,\n",
    "                                        mask_zero=False)(input_text)\n",
    "\n",
    "\n",
    "#BAG_OF_WORDS\n",
    "bag = keras.layers.Lambda(lambda x: K.sum(x, axis=2), output_shape=(nb_replique_max, embedding_output_dim,))(embedding_layer)\n",
    "\n",
    "\n",
    "#INPUTBIS\n",
    "input_incise = keras.layers.Input((nb_replique_max, embedding_incise_dim, ), name = \"aux\")\n",
    "\n",
    "\n",
    "#MERGING\n",
    "merged = keras.layers.concatenate([bag, input_incise])\n",
    "\n",
    "\n",
    "#LSTM\n",
    "units = nb_personnages + 1\n",
    "#lstm = keras.layers.LSTM(units, return_sequences=True) (merged)\n",
    "lstm = keras.layers.LSTM(units, return_sequences=True) (input_incise)\n",
    "\n",
    "#MODEL\n",
    "\n",
    "sample = np.array([[int(col != 0) for col in lig] for lig in people])\n",
    "\n",
    "#OUT\n",
    "out = keras.layers.Activation('softmax') (lstm)\n",
    "\n",
    "model = keras.Model(inputs = [input_text, input_incise] , outputs = out)\n",
    "#model = keras.Model(inputs = input_incise, outputs = out)\n",
    "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', \n",
    "              metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "\n",
    "\n",
    "#data_zero = np.zeros((discussion_int, l_max, longueur))\n",
    "x_train = [data,Z]\n",
    "y_train = Y\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=1,\n",
    "          sample_weight = sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_fake[:] = 0\n",
    "Z_fake[:, :, 5] = 1\n",
    "Z_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.10975358, 0.10491947, 0.10285795, 0.11516447, 0.11314581, 0.1270309 , 0.10674074,\n",
       "         0.11715734, 0.1032297 ],\n",
       "        [0.10882805, 0.09977198, 0.09818061, 0.11740898, 0.11424237, 0.13831694, 0.10388377,\n",
       "         0.12004247, 0.09932473],\n",
       "        [0.10824458, 0.0957982 , 0.09556642, 0.11858702, 0.11502451, 0.14586535, 0.10205936,\n",
       "         0.12141627, 0.09743826],\n",
       "        [0.10790848, 0.09284022, 0.09411175, 0.11914074, 0.11570087, 0.15080579, 0.10089497,\n",
       "         0.12206629, 0.09653091],\n",
       "        [0.10773817, 0.09067175, 0.0933149 , 0.11934382, 0.11632665, 0.1540028 , 0.10014976,\n",
       "         0.12236209, 0.09609   ],\n",
       "        [0.1076732 , 0.08908736, 0.09289556, 0.11936473, 0.11690675, 0.1560497 , 0.09967295,\n",
       "         0.12248108, 0.09586864]],\n",
       "\n",
       "       [[0.10975358, 0.10491947, 0.10285795, 0.11516447, 0.11314581, 0.1270309 , 0.10674074,\n",
       "         0.11715734, 0.1032297 ],\n",
       "        [0.10882805, 0.09977198, 0.09818061, 0.11740898, 0.11424237, 0.13831694, 0.10388377,\n",
       "         0.12004247, 0.09932473],\n",
       "        [0.10824458, 0.0957982 , 0.09556642, 0.11858702, 0.11502451, 0.14586535, 0.10205936,\n",
       "         0.12141627, 0.09743826],\n",
       "        [0.10790848, 0.09284022, 0.09411175, 0.11914074, 0.11570087, 0.15080579, 0.10089497,\n",
       "         0.12206629, 0.09653091],\n",
       "        [0.10773817, 0.09067175, 0.0933149 , 0.11934382, 0.11632665, 0.1540028 , 0.10014976,\n",
       "         0.12236209, 0.09609   ],\n",
       "        [0.1076732 , 0.08908736, 0.09289556, 0.11936473, 0.11690675, 0.1560497 , 0.09967295,\n",
       "         0.12248108, 0.09586864]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5, 5, 5, 5],\n",
       "       [5, 5, 5, 5, 5, 5]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = model.predict(Z_fake)\n",
    "\n",
    "display(prediction)\n",
    "display(prediction.argmax(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 6, 12), (2, 6, 5))"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_zero.shape, Z_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mrs. Bennet': 1, 'Mr. Bennet': 2, 'Elizabeth Bennet': 3, 'Kitty Bennet': 4, 'Lydia Bennet': 5, 'Mr. Bingley': 6, 'Mr. Darcy': 7, 'Jane Bennet': 8, 'Charlotte': 9, 'Mary Bennet': 10, 'young male Lucas': 11, 'Sir William': 12, 'Caroline Bingley': 13, 'Catherine and Lydia Bennet': 14, 'Louisa Hurst': 15, 'Mr. Hurst': 16, 'Mr. Collins': 17, 'NOTANUTTERANCE': 18, 'Mr. Wickham': 19, 'Mr. Denny': 20, 'Mrs. Gardiner': 21, 'Maria': 22, 'Lady Catherine': 23, 'Colonel Fitzwilliam': 24, 'Kitty and Lydia Bennet': 25, 'Mrs. Reynolds': 26, 'Mr. Gardiner': 27, 'Elizabeth Bennet and Mr. Darcy': 28, 'Mrs. Hill': 29, 'The Butler': 30, 'Mrs. Phillips': 31, 'UNSURE': 32, 'Mr. Bennet and Mr. Collins': 33}\n"
     ]
    }
   ],
   "source": [
    "print(dictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "bon = 0\n",
    "for num in range(0,61):\n",
    "    res = model.predict([data[num:num+1] ,Z[num:num+1] ])\n",
    "    #print(res[0,0,:])\n",
    "    for d in range(0,l_max):\n",
    "        if(sample[num,d]!=0):\n",
    "            if(np.argmax(Y[num,d,:]) == np.argmax(res[0,d,:])):\n",
    "                bon += 1\n",
    "                print(np.argmax(Y[num,d,:]))\n",
    "print(bon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2], [2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2]]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "disc = []\n",
    "for num in range(0,61):\n",
    "    res = model.predict([data[num:num+1] ,Z[num:num+1] ])\n",
    "    #print(res[0,0,:])\n",
    "    for d in range(0,l_max):\n",
    "        if(sample[num,d]!=0):\n",
    "            disc.append(np.argmax(res[0,d,:]))\n",
    "    pred.append(disc)\n",
    "    disc = []\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 5)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_fake = [data_zero, Z_fake]\n",
    "model = keras.Model(inputs = [input_text, input_incise] , outputs = lstm)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'],\n",
    "              sample_weight_mode='temporal')\n",
    "# model.fit(x_fake, Y_fake,\n",
    "#           epochs=20,\n",
    "#           batch_size=1,\n",
    "#           sample_weight = np.ones((discussion_int,l_max)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         0.         0.7615942 -0.         0.         0.\n",
      "  0.         0.7615942  0.         0.        -0.7615942  0.\n",
      " -0.7615942 -0.7615942 -0.        -0.        -0.7615942 -0.\n",
      " -0.        -0.         0.         0.         0.         0.\n",
      "  0.         0.        -0.7615942  0.         0.        -0.\n",
      "  0.7615942  0.         0.        -0.       ]\n",
      "[ 0.         0.         0.9640276 -0.         0.         0.\n",
      "  0.         0.7615942  0.         0.        -0.7615942  0.\n",
      " -0.9640276 -0.9640276 -0.        -0.        -0.7615942 -0.\n",
      " -0.        -0.         0.         0.         0.         0.\n",
      "  0.         0.        -0.9640276  0.         0.        -0.\n",
      "  0.9640276  0.         0.        -0.       ]\n",
      "[ 0.         0.         0.9950547 -0.         0.         0.\n",
      "  0.         0.7615942  0.         0.        -0.7615942  0.\n",
      " -0.9950547 -0.9950547 -0.        -0.        -0.7615942 -0.\n",
      " -0.        -0.         0.         0.         0.         0.\n",
      "  0.         0.        -0.9950547  0.         0.        -0.\n",
      "  0.9950547  0.         0.        -0.       ]\n",
      "[ 0.         0.         0.9993292 -0.         0.         0.\n",
      "  0.         0.7615942  0.         0.        -0.7615942  0.\n",
      " -0.9993292 -0.9993292 -0.        -0.        -0.7615942 -0.\n",
      " -0.        -0.         0.         0.         0.         0.\n",
      "  0.         0.        -0.9993292  0.         0.        -0.\n",
      "  0.9993292  0.         0.        -0.       ]\n",
      "[ 0.          0.          0.99990916 -0.          0.          0.\n",
      "  0.          0.7615942   0.          0.         -0.7615942   0.\n",
      " -0.99990916 -0.99990916 -0.         -0.         -0.7615942  -0.\n",
      " -0.         -0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.99990916  0.          0.         -0.\n",
      "  0.99990916  0.          0.         -0.        ]\n",
      "[2, 2, 2, 2, 2]\n",
      "[18, 3, 6, 28, 11]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "predicted = []\n",
    "real = []\n",
    "faux = model.predict([data_zero[n:n+1],Z_fake[n:n+1]])\n",
    "for i in range(0,5):#l_max):\n",
    "    print(faux[0,i,:])\n",
    "    predicted.append(np.argmax(faux[0,i,:]))\n",
    "    real.append(np.argmax(Y_fake[n,i,:]))\n",
    "print(predicted)\n",
    "print(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(data_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
