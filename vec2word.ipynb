{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for dataset file /Users/perceval/Developpement/Data/alice-in-wonderland/alice.txt\n"
     ]
    }
   ],
   "source": [
    "# Parser for command line arguments\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"MAX_SEQUENCE_LENGTH\": 200,\n",
    "    \"MAX_NB_WORDS\": 10000,\n",
    "    \"EMBEDDING_DIM\": 100,\n",
    "    \"VALIDATION_SPLIT\": 0.2,\n",
    "    \"NUM_LSTM\": 64,\n",
    "    \"PATIENCE\": 2,\n",
    "    \"EPOCHS\": 8,\n",
    "    \"BATCH_SIZE\": 16, # not large because every sample is 300 words long\n",
    "    \"LR\": 0.001,\n",
    "    \"MSE_LOSS_WEIGHT\": 500,\n",
    "}\n",
    "\n",
    "TIME_STR = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_MODEL_PATH = os.path.join('./output/', \"model-{}.h5\".format(TIME_STR))\n",
    "CHECKPOINT_PATH = os.path.join('./output/', \"model-{}-checkpoint.h5\".format(TIME_STR))\n",
    "GLOVE_PATH = '/Users/perceval/Developpement/Data/glove.6B.100d/glove.6B.100d.txt'\n",
    "TFLOGS_PATH = './output/'\n",
    "DATA_PATH = '/Users/perceval/Developpement/Data/alice-in-wonderland/alice.txt'\n",
    "SPLIT_REGEX = '[^a-zA-Z](‘)|(‘)[^a-zA-Z]|(?:\\s|\\n)+|(\\d+\\.\\d*)|([\".!,;:-])'\n",
    "\n",
    "\n",
    "def make_data(dataset_path):\n",
    "    \"\"\"Load reuters datasets, categories, and preprocesses the texts\"\"\"\n",
    "\n",
    "    print(\"Searching for dataset file {}\".format(dataset_path))\n",
    "\n",
    "    with open(dataset_path) as f:\n",
    "        book = f.read()\n",
    "\n",
    "    book_tokens = [w for w in re.split(SPLIT_REGEX, book[576:].lower())\n",
    "                   if w is not None and w != '']\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_tokens = [t for t in (stemmer.stem(w) for w in book_tokens) if len(t) > 0]\n",
    "\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    for t in stemmed_tokens:\n",
    "        current_sentence.append(t)\n",
    "        if t in ['.', '!', ';', '...', '\"', '‘']:\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    if len(current_sentence) > 0:\n",
    "        sentences.append(current_sentence)\n",
    "\n",
    "    train_sentences = [' '.join(s) for s in sentences]\n",
    "\n",
    "    # Fit the tokenizer on train texts\n",
    "    tokenizer = Tokenizer(num_words=params['MAX_NB_WORDS'])\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "    # Convert them to indices and truncate them if they are too large\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_sentences)#, params['MAX_SEQUENCE_LENGTH'])\n",
    "    # test_seqs = pad_sequences(tokenizer.texts_to_sequences(test_sentences), params['MAX_SEQUENCE_LENGTH'])\n",
    "\n",
    "    return train_seqs, tokenizer.word_index\n",
    "\n",
    "\n",
    "def make_embedding_weights(_word_index, glove_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((params['MAX_NB_WORDS'], params['EMBEDDING_DIM']))\n",
    "    for word, i in _word_index.items():\n",
    "        if i < params['MAX_NB_WORDS']:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def custom_loss(true_y, preds):\n",
    "    int_labels = K.argmax(true_y, axis=2)  # -> 300 * 1887\n",
    "\n",
    "    true_embeddings = tf.nn.embedding_lookup(embedding_layer.embeddings, int_labels)  # -> 300 * 1887, 100\n",
    "    diff_loss = K.mean(K.square(encoded - true_embeddings), axis=None)\n",
    "    cat_crossentropy_loss = K.categorical_crossentropy(true_y, preds)\n",
    "    return params[\"MSE_LOSS_WEIGHT\"] * diff_loss + cat_crossentropy_loss\n",
    "\n",
    "\n",
    "train_sequences, word_index = make_data(DATA_PATH)\n",
    "\n",
    "index_word = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_weights = make_embedding_weights(word_index, GLOVE_PATH)\n",
    "\n",
    "X_train = np.copy(train_sequences)\n",
    "X_train[:, 1:] = X_train[:, :-1]\n",
    "X_train[:, 0] = 0\n",
    "\n",
    "Y_train = np.zeros((len(train_sequences) * params['MAX_SEQUENCE_LENGTH'],\n",
    "                    params['MAX_NB_WORDS']), dtype=int)\n",
    "Y_train[np.arange(len(Y_train)), train_sequences.reshape(-1)] = 1\n",
    "Y_train = Y_train.reshape((len(train_sequences),\n",
    "                           params['MAX_SEQUENCE_LENGTH'],\n",
    "                           params['MAX_NB_WORDS']))\n",
    "\n",
    "inputs = keras.layers.Input((params['MAX_SEQUENCE_LENGTH'],))\n",
    "embedding_layer = keras.layers.Embedding(input_dim=params['MAX_NB_WORDS'], output_dim=params['EMBEDDING_DIM'])\n",
    "embedding = embedding_layer(inputs)\n",
    "out = keras.layers.LSTM(params['NUM_LSTM'], return_sequences=True)(embedding)\n",
    "out = keras.layers.Dropout(0.2)(out)\n",
    "encoded = keras.layers.Dense(params['EMBEDDING_DIM'], activation='relu')(out)\n",
    "out = keras.layers.Dense(params['MAX_NB_WORDS'], activation='softmax')(encoded)\n",
    "\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                             mode='min')\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=TFLOGS_PATH, histogram_freq=0, batch_size=params['BATCH_SIZE'],\n",
    "                                          write_graph=True, write_grads=False, write_images=False, embeddings_freq=0,\n",
    "                                          embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs,\n",
    "                           outputs=out)\n",
    "model.compile(loss=[custom_loss],\n",
    "              metrics=['acc'],\n",
    "              optimizer='adam')\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_split=params['VALIDATION_SPLIT'],\n",
    "    batch_size=params['BATCH_SIZE'],\n",
    "    epochs=params['EPOCHS'],\n",
    "    callbacks=[checkpoint, tensorboard],\n",
    ")\n",
    "\n",
    "model.save(OUT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
