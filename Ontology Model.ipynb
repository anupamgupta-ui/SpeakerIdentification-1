{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ontology & Parsing \n",
    "===\n",
    "\n",
    "This notebook focuses on parsing and the dataset and making inferences through the ontology engine\n",
    "\n",
    "The results are saved to be reused with the Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import tqdm # progress bars, really nice to save some notebook space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# allows the developement of Python files and their automatic reloading as they are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "%aimport prolog\n",
    "%aimport preprocessing\n",
    "%aimport parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People list\n",
    "\n",
    "In the future, we may want to extract the people list dynamically, rather than hardcoding it before processing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Extract people... ?\n",
    "# \n",
    "# from interval import interval\n",
    "# \n",
    "# text = \" \".join(s[\"source\"] for s in dataset)\n",
    "# \n",
    "# # Find Cap names\n",
    "# cap_names = list(re.finditer(\"(?:[a-z][,:; ]+)([A-Z][a-z]*(?:\\s+[A-Z][a-z]*)*)\", text))\n",
    "# # Find Mr., Mrs., Sir. names\n",
    "# title_names = list(re.finditer(\"([Mm](?:rs?\\.?|iss\\.?|[Ss]ir)\\s+[A-Z][a-z]*(?:\\s+[A-Z][a-z]*)*)\", text))\n",
    "# \n",
    "# characters_spans = interval(*[n.span(1) for n in cap_names] + [n.span(0) for n in title_names])\n",
    "# \n",
    "# # {text[int(s[0]):int(s[1])]: ((int(s[0]), int(s[1]), text[int(s[0])-10:int(s[1])+20])) for s in characters_spans}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we just hard code the known list of protagonists (corrected from the given list in the article)\n",
    "\n",
    "`people_list = [...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_list = pickle.load(open(\"corpus/people.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_code_to_name = {p[\"code\"]: p[\"main\"] for p in people_list}\n",
    "people_name_to_code = {p['main'].strip('s'): p['code'] for p in people_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book utterances splitting and annotations matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Almost match :\n",
      "My dear Elizabeth_Bennet, I have the highest opinion in the world of your excellent judgment in all matters within the scope of your understanding, but permit me to say that there must be a wide difference between the established forms of ceremony amongst the laity, and those which regulate the clergy; for give me leave to observe that I consider the clerical office as equal in point of dignity with the highest rank in the kingdom -- provided that a proper humility of behaviour is at the same time maintained. You must therefore allow me to follow the dictates of my conscience on this occasion, which leads me to perform what I look on as a point of duty. Pardon me for neglecting to profit by your advice, which on every other subject shall be my constant guide, though in the case before us I consider myself more fitted by education and habitual study to decide on what is right than a young lady like yourself. [X] apology, [X] Hunsford, [X] Lady_Catherine.\n",
      "My dear Elizabeth_Bennet, I have the highest opinion in the world of your excellent judgment in all matters within the scope of your understanding, but permit me to say that there must be a wide difference between the established forms of ceremony amongst the laity, and those which regulate the clergy; for give me leave to observe that I consider the clerical office as equal in point of dignity with the highest rank in the kingdom -- provided that a proper humility of behaviour is at the same time maintained. You must therefore allow me to follow the dictates of my conscience on this occasion, which leads me to perform what I look on as a point of duty. Pardon me for neglecting to profit by your advice, which on every other subject shall be my constant guide, though in the case before us I consider myself more fitted by education and habitual study to decide on what is right than a young lady like yourself. apology, Hunsford, Lady_Catherine.\n",
      "--\n",
      "Almost match :\n",
      "delightful, [X] charming,\n",
      "delightful, charming,\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Transforms a text in utterances\n",
    "utterances = preprocessing.build_dataset(\n",
    "    text_file='corpus/PRIDPREJ_NONEWLINE_Organize_v2.txt',\n",
    "    people=people_list\n",
    ")\n",
    "\n",
    "# Match them with the labelled dataset\n",
    "dataset = preprocessing.match_with_annoted_file(\n",
    "    path='corpus/REAL_ALL_CONTENTS_PP.txt',\n",
    "    utterances=utterances,\n",
    "    people=people_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'begin': 0,\n",
       " 'discussion_index': 0,\n",
       " 'end': 109,\n",
       " 'only_utterance_article': 'My dear Mr_Bennet, [X] have you heard that Netherfield Park is let at last?',\n",
       " 'only_utterance_us': 'My dear Mr_Bennet, [X] have you heard that Netherfield Park is let at last?',\n",
       " 'parts': [{'text': 'My dear Mr_Bennet,', 'utterance': True},\n",
       "  {'text': ' said his lady to him one day, ', 'utterance': False},\n",
       "  {'text': 'have you heard that Netherfield Park is let at last?',\n",
       "   'utterance': True}],\n",
       " 'source': \"``My dear Mr_Bennet,'' said his lady to him one day, ``have you heard that Netherfield Park is let at last?''\",\n",
       " 'target': 'Mrs_Bennet'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar parsing\n",
    "\n",
    "Let's detect some features (relations, gender, name) in the narration/utterance parts, with the stanford parser   \n",
    "Each utterance/narration part will give us a list of names and ontology properties about the subject and its destinator\n",
    "\n",
    "The following cell took us ~ 3h to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = parsing.load_parser('../stanford-parser-full-2017-06-09/')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "people_main = [p['main'] for p in people_list]\n",
    "\n",
    "for sample in tqdm.tqdm(dataset):\n",
    "    for part in sample[\"parts\"]:\n",
    "        if part[\"utterance\"]:\n",
    "            part[\"speaker_features\"], part[\"dest_features\"] = parsing.extract_features_from_utterance(part[\"text\"], parser, stemmer, people_name_to_code)\n",
    "        else:\n",
    "            part[\"speaker_features\"], part[\"dest_features\"] = parsing.extract_features_from_narration(part[\"text\"], parser, stemmer, people_name_to_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small demo of the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- triples ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1, 'VBD'), 'dobj', (2, 'NNP')),\n",
       " ((2, 'NNP'), 'acl:relcl', (5, 'VBD')),\n",
       " ((5, 'VBD'), 'dobj', (6, 'PRP')),\n",
       " ((5, 'VBD'), 'nsubj', (4, 'WP')),\n",
       " ((5, 'VBD'), 'nmod', (9, 'NN')),\n",
       " ((9, 'NN'), 'nmod', (12, 'NNS')),\n",
       " ((12, 'NNS'), 'nmod:poss', (11, 'PRP$')),\n",
       " ((12, 'NNS'), 'case', (10, 'IN')),\n",
       " ((9, 'NN'), 'det', (8, 'DT')),\n",
       " ((9, 'NN'), 'case', (7, 'IN')),\n",
       " ((1, 'VBD'), 'nsubj', (0, 'NNP'))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stemmed ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['XXX',\n",
       " 'observ',\n",
       " 'Mary_Bennet',\n",
       " '',\n",
       " 'who',\n",
       " 'piqu',\n",
       " 'herself',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'solid',\n",
       " 'of',\n",
       " 'her',\n",
       " 'reflect']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, (5, 'XXX')),\n",
       " (1, (0, 'observ')),\n",
       " (2, (3, 'Mary_Bennet')),\n",
       " (3, (0, '')),\n",
       " (4, (3, 'who')),\n",
       " (5, (0, 'piqu')),\n",
       " (6, (1, 'herself')),\n",
       " (7, (0, 'upon')),\n",
       " (8, (0, 'the')),\n",
       " (9, (2, 'solid')),\n",
       " (10, (0, 'of')),\n",
       " (11, (0, 'her')),\n",
       " (12, (1, 'reflect'))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-parsing with ['Mary_Bennet', 'who', 'piqu', 'herself', 'upon', 'the', 'solid', 'of', 'her', 'reflect', 'observ', '']\n",
      "--- triples ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((10, 'VBD'), 'dobj', (11, 'NNS')),\n",
       " ((10, 'VBD'), 'nsubj', (0, 'NNP')),\n",
       " ((0, 'NNP'), 'acl:relcl', (2, 'VBD')),\n",
       " ((2, 'VBD'), 'dobj', (3, 'PRP')),\n",
       " ((2, 'VBD'), 'nsubj', (1, 'WP')),\n",
       " ((2, 'VBD'), 'nmod', (6, 'NN')),\n",
       " ((6, 'NN'), 'nmod', (9, 'NNS')),\n",
       " ((9, 'NNS'), 'nmod:poss', (8, 'PRP$')),\n",
       " ((9, 'NNS'), 'case', (7, 'IN')),\n",
       " ((6, 'NN'), 'det', (5, 'DT')),\n",
       " ((6, 'NN'), 'case', (4, 'IN'))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stemmed ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mary_Bennet',\n",
       " 'who',\n",
       " 'piqu',\n",
       " 'herself',\n",
       " 'upon',\n",
       " 'the',\n",
       " 'solid',\n",
       " 'of',\n",
       " 'her',\n",
       " 'reflect',\n",
       " 'observ',\n",
       " '']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, (5, 'Mary_Bennet')),\n",
       " (1, (3, 'who')),\n",
       " (2, (0, 'piqu')),\n",
       " (3, (1, 'herself')),\n",
       " (4, (0, 'upon')),\n",
       " (5, (0, 'the')),\n",
       " (6, (2, 'solid')),\n",
       " (7, (0, 'of')),\n",
       " (8, (0, 'her')),\n",
       " (9, (1, 'reflect')),\n",
       " (10, (0, 'observ')),\n",
       " (11, (3, ''))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0, (3, 'Mary_Bennet')),\n",
       " (1, (1, 'who')),\n",
       " (2, (0, 'piqu')),\n",
       " (3, (1, 'herself')),\n",
       " (4, (0, 'upon')),\n",
       " (5, (0, 'the')),\n",
       " (6, (3, 'solid')),\n",
       " (7, (0, 'of')),\n",
       " (8, (2, 'her')),\n",
       " (9, (2, 'reflect')),\n",
       " (10, (0, 'observ')),\n",
       " (11, (3, ''))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(({'Mary_Bennet'}, set()), (set(), set()))"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing.extract_features_from_narration(\" observed Mary_Bennet, who piqued herself upon the solidity of her reflections, \", parser, stemmer, people_name_to_code, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- triples ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1, 'NNP'), 'case', (0, 'IN')),\n",
       " ((5, 'VBN'), 'nmod', (1, 'NNP')),\n",
       " ((5, 'VBN'), 'auxpass', (3, 'VBP')),\n",
       " ((5, 'VBN'), 'nsubjpass', (4, 'PRP'))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stemmed ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'dear'), (1, 'Mr_Bennet'), (2, ''), (3, 'are'), (4, 'you'), (5, 'ok')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((set(), set()), ({'Mr_Bennet'}, set()))"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing.extract_features_from_utterance(\"dear Mr_Bennet, are you ok ?\", parser, stemmer, people_name_to_code, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset, open(\"corpus/dataset-parsed.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prolog engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define facts, mainly family relations and genders using Prolog\n",
    "\n",
    "We could think in the future to define in the same way alias relationships and use Bayesian networks instead\n",
    "to have more sensitive system, but this way works well enough for us now\n",
    "\n",
    "`facts = [...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = \"\"\"\n",
    "status(mrs_annesley,female).\n",
    "status(elizabeth_bennet,female).\n",
    "status(jane_bennet,female).\n",
    "status(lydia_bennet,female).\n",
    "status(kitty_bennet,female).\n",
    "status(mary_bennet,female).\n",
    "status(mrs_bennet,female).\n",
    "status(mr_bennet,male).\n",
    "status(mr_bingley,male).\n",
    "status(caroline_bingley,female).\n",
    "status(charlotte,female).\n",
    "status(captain_carter,female).\n",
    "status(mr_collins,male).\n",
    "status(lady_catherine,female).\n",
    "status(mr_chamberlayne,female).\n",
    "status(dawson,female).\n",
    "status(mr_denny,female).\n",
    "status(mr_darcy,male).\n",
    "status(old_mr_darcy,female).\n",
    "status(lady_anne_darcy,female).\n",
    "status(georgiana_darcy,female).\n",
    "status(colonel_fitzwilliam,male).\n",
    "status(colonel_forster,female).\n",
    "status(miss_grantley,female).\n",
    "status(mrs_gardiner,female).\n",
    "status(mr_gardiner,male).\n",
    "status(william_goulding,female).\n",
    "status(haggerston,female).\n",
    "status(mrs_hill,female).\n",
    "status(mrs_jenkinson,female).\n",
    "status(mr_jones,female).\n",
    "status(miss_mary_king,female).\n",
    "status(mrs_long,female).\n",
    "status(lady_lucas,female).\n",
    "status(maria_lucas,female).\n",
    "status(mr_hurst,female).\n",
    "status(louisa_hurst,female).\n",
    "status(lady_metcalfe,female).\n",
    "status(mr_morris,female).\n",
    "status(mrs_nicholls,female).\n",
    "status(mr_philips,male).\n",
    "status(miss_pope,female).\n",
    "status(mr_pratt,male).\n",
    "status(mrs_reynolds,female).\n",
    "status(mr_robinson,female).\n",
    "status(mr_stone,female).\n",
    "status(miss_watson,female).\n",
    "status(old_mr_wickham,female).\n",
    "status(sir_william,male).\n",
    "status(anne_de_bourgh,female).\n",
    "status(mr_wickham,male).\n",
    "status(mrs_philips,female).\n",
    "status(young_lucas,male).\n",
    "status(the_butler,male).\n",
    "\n",
    "status(elizabeth_bennet,female).\n",
    "related(jane_bennet,elizabeth_bennet,sister).\n",
    "related(mary_bennet,elizabeth_bennet,sister).\n",
    "related(lydia_bennet,elizabeth_bennet,sister).\n",
    "related(kitty_bennet,elizabeth_bennet,sister).\n",
    "related(mr_bennet,elizabeth_bennet,father).\n",
    "related(mrs_bennet,elizabeth_bennet,mother).\n",
    "related(mr_collins,charlotte,husband).\n",
    "related(mr_collins,mr_bennet,brother).\n",
    "related(mr_bingley,caroline_bingley,siblings).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"prolog_engine\" in globals() and not prolog_engine.closed:\n",
    "    prolog_engine.close()\n",
    "prolog_engine = prolog.Prolog('./family.pl')\n",
    "prolog_engine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prolog_engine.assert_facts(facts) # input the facts\n",
    "prolog_engine.query(\"abolish_all_tables.\") # reset the cache, needed for the circular rules dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X': 'mrs_bennet'}]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prolog_engine.query('related(X, mr_bennet, wife).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell took us ~ 30min to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [02:51<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "source": [
    "dl_dataset = []\n",
    "for utterance in tqdm.tqdm(dataset[len(dl_dataset):]):\n",
    "    parts = utterance['parts']\n",
    "    parts_count = len(parts)\n",
    "    \n",
    "    speaker = utterance['target']\n",
    "    discussion_index = utterance['discussion_index']\n",
    "    \n",
    "    utterance_text = re.sub('\\[X\\]', '', utterance['only_utterance_us'])\n",
    "    incises = [part for part in parts if not part['utterance']]\n",
    "    \n",
    "    concat_incise = \"\".join(part[\"text\"] for part in incises)\n",
    "    \n",
    "    potential_targets, dest_targets = [], []\n",
    "    \n",
    "    # Merge all the information gathered during the parsing\n",
    "    # of each part of the utterance line\n",
    "    for part in parts:\n",
    "        if part[\"speaker_features\"][0] is not None:\n",
    "            potential_targets.extend(list(part[\"speaker_features\"][0]))\n",
    "        if part[\"speaker_features\"][1] is not None and len(part[\"speaker_features\"][1]) > 0:\n",
    "            prolog_results = prolog_engine.query(\",\".join(part[\"speaker_features\"][1])+\".\")\n",
    "            if prolog_results:\n",
    "                potential_targets.extend(list(set([people_code_to_name[r['X']] for r in prolog_results if 'X' in r and r['X'] in people_code_to_name])))\n",
    "            else:\n",
    "                # try to reset the engine if there is a problem\n",
    "                prolog_engine.query(\"X=1.\")\n",
    "        if part[\"dest_features\"][0] is not None:\n",
    "            dest_targets.extend(list(part[\"dest_features\"][0]))\n",
    "            \n",
    "        if part[\"dest_features\"][1] is not None and len(part[\"dest_features\"][1]) > 0:\n",
    "            prolog_results = prolog_engine.query(\",\".join(part[\"dest_features\"][1])+\".\")\n",
    "            if prolog_results:\n",
    "                dest_targets.extend(list(set([people_code_to_name[r['U']] for r in prolog_results if 'U' in r and r['U'] in people_code_to_name])))\n",
    "            else:\n",
    "                # try to reset the engine if there is a problem\n",
    "                prolog_engine.query(\"X=1.\")\n",
    "\n",
    "    dl_dataset.append((discussion_index, utterance_text, concat_incise, potential_targets, dest_targets, speaker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset in multiple discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_discussions = []\n",
    "current_discussion = []\n",
    "current_discussion_index = None\n",
    "for sample in dl_dataset:\n",
    "    if sample[0] != current_discussion_index and len(current_discussion) > 0:\n",
    "        dataset_discussions.append(current_discussion)\n",
    "        current_discussion = []\n",
    "    current_discussion_index = sample[0]\n",
    "    current_discussion.append(sample[1:])\n",
    "if len(current_discussion) > 0:\n",
    "    dataset_discussions.append(current_discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dataset_discussions, open(\"corpus/dataset-dl.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction only based on parsing and ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "correct_answers = 0\n",
    "people_names = [p['main'] for p in people_list]\n",
    "for discussion in dataset_discussions:\n",
    "    for sample in discussion:\n",
    "        if len(sample[2]) > 0 and np.random.choice(list(sample[2]), 1) == sample[4]:\n",
    "            correct_answers += 1\n",
    "print(\"Accuracy: {}\".format(correct_answers/len(dl_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction based on parsing and ontology and nearby speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_people_predictions(sets, weights=[]):\n",
    "    \"\"\"Poll on the potential characters in the `sets`, weighted by the `weights`\"\"\"\n",
    "    weights = weights[:len(sets)] + [1] * (len(sets)-len(weights))\n",
    "    sets_weight = [(s, w) for s, w in zip(sets, weights) if s is not None and len(s) > 0]\n",
    "    \n",
    "    potential_targets = {}\n",
    "    for s, w in sets_weight:\n",
    "        w = w if w is not None else 1\n",
    "        for p in s:\n",
    "            potential_targets[p] = potential_targets.get(p, 0) + w\n",
    "    if len(potential_targets) == 0:\n",
    "        return None\n",
    "    return max(potential_targets, key=lambda p: potential_targets[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5491551459293394\n"
     ]
    }
   ],
   "source": [
    "correct_answers = 0\n",
    "people_names = [p['main'] for p in people_list]\n",
    "for discussion in dataset_discussions:\n",
    "    discussion_people = set()\n",
    "    previous_predictions = [] \n",
    "    for i, sample in enumerate(discussion):\n",
    "        if len(sample[2]) == 1:\n",
    "            current_prediction = next(iter(sample[2]))\n",
    "            discussion_people.add(current_prediction)\n",
    "        else:\n",
    "            current_prediction = merge_people_predictions([sample[2], \n",
    "                                                            discussion_people,\n",
    "                                                           \n",
    "                                                            # n-1, n+1\n",
    "                                                            # We consider that a user will not speak twice in a row\n",
    "                                                            # so we penalize the speakers of the n+1, n-1 utterances\n",
    "                                                            discussion[i-1][3] if i >= 1 else None,\n",
    "                                                            discussion[i+1][3] if i < len(discussion) - 1 else None,\n",
    "                                                            discussion[i-1][2] if i >= 1 else None,\n",
    "                                                            discussion[i+1][2] if i < len(discussion) - 1 else None,\n",
    "                                                           \n",
    "                                                            # n-2, n+2\n",
    "                                                            discussion[i-2][3] if i >= 1 else None,\n",
    "                                                            discussion[i+2][3] if i < len(discussion) - 2 else None,\n",
    "                                                            discussion[i-2][2] if i >= 2 else None,\n",
    "                                                            discussion[i+2][2] if i < len(discussion) - 2 else None,\n",
    "                                                           \n",
    "                                                            # n-3, n+3\n",
    "                                                            # same thing a n-1, n+1, but with a smaller weight\n",
    "                                                            # and since we are getting further from the current utterance,\n",
    "                                                            # we only focus on positive influences ie destinator of n-3, n+3...\n",
    "                                                            discussion[i-3][3] if i >= 3 else None,\n",
    "                                                            discussion[i+3][3] if i < len(discussion) - 3 else None,\n",
    "                                                           \n",
    "                                                            # n-4, n+4\n",
    "                                                            # ...and speaker of n-4, n+4\n",
    "                                                            discussion[i-4][2] if i >= 4 else None,\n",
    "                                                            discussion[i+4][2] if i < len(discussion) - 4 else None], weights=[4, 4,\n",
    "                                                                                         2, 2, -2, -2,\n",
    "                                                                                         -1,-1, 1, 1,\n",
    "                                                                                         1, 1,\n",
    "                                                                                         1, 1])\n",
    "        if current_prediction is not None:\n",
    "            previous_predictions.append(current_prediction)\n",
    "        else:\n",
    "            previous_predictions.append(None)\n",
    "            \n",
    "        if current_prediction is not None and current_prediction == sample[4]:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "print(\"Accuracy: {}\".format(correct_answers/len(dl_dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
