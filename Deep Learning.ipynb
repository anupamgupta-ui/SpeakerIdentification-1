{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perceval/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/perceval/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=100, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"UTTERANCE_LENGTH\": 60,\n",
    "    \"DISCUSSION_LENGTH\": 57, #60,\n",
    "    \"NARRATION_LENGTH\": 30,\n",
    "    \"WORD_EMBEDDING_DIM\": 100,\n",
    "    \"PEOPLE_EMBEDDING_DIM\": 64,\n",
    "    \"RECURRENT_UNITS_COUNT\": 32,\n",
    "    \"EPOCHS\": 8,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"VALIDATION_SPLIT\": 0.3,\n",
    "    \"TEST_SPLIT\": 0.2,\n",
    "}\n",
    "\n",
    "\n",
    "TIME_STR = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_MODEL_PATH = os.path.join('./output/', \"model-{}.h5\".format(TIME_STR))\n",
    "CHECKPOINT_PATH = os.path.join('./output/', \"model-{}-checkpoint.h5\".format(TIME_STR))\n",
    "GLOVE_PATH = '/Users/perceval/Developpement/Data/glove.6B.100d/glove.6B.100d.txt'\n",
    "DATA_PATH = 'corpus/dataset_dl.pkl'\n",
    "SPLIT_REGEX = r\"[@_\\w]+|['.,!?;]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pickle.load(open(\"corpus/people.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"corpus/dataset-dl.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My dear Mr_Bennet,  have you heard that Netherfield Park is let at last?',\n",
       " ' said his lady to him one day, ',\n",
       " ['Charlotte', 'Mrs_Bennet'],\n",
       " ['Mr_Bennet'],\n",
       " 'Mrs_Bennet')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and process it (again !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to stem the data, to make the vocabulary denser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentence(stemmer, sentence, people_names):\n",
    "    tokens = re.findall(SPLIT_REGEX, sentence)\n",
    "    return [stemmer.stem(word) if word not in people_names else word for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr_Bennet', 'is', 'with', 'Mrs_Bennet', \"'\", 's', 'daughter']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_sentence(SnowballStemmer('english'), \"Mr_Bennet is with Mrs_Bennet's daughter\", [p['main'] for p in people])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(list, length, default):\n",
    "    \"\"\"Pads the `list`, adding `default` as many times as necessary to reach the provided `length`\"\"\"\n",
    "    if length is None:\n",
    "        return list\n",
    "    return  [list[i] if i < len(list) else default for i in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, people, verbose=2, utterance_length=8, narration_length=None, discussion_length=None):\n",
    "    \"\"\"Tokenizes, stems and transform into to ids the provided `dataset`\n",
    "    \"\"\"\n",
    "    people_main = [p['main'] for p in people]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    stemmed_samples = [\n",
    "        [(stem_sentence(stemmer, utterance[0], people_main), # utterance part\n",
    "          stem_sentence(stemmer, utterance[1], people_main), # narration part\n",
    "          utterance[2], # potential subjects\n",
    "          utterance[3], # potential destinator\n",
    "          utterance[4]) # label\n",
    "         for utterance in discussion]\n",
    "        for discussion in (tqdm(dataset, desc=\"Text words stemming\")\n",
    "                           if verbose > 1 else dataset)]\n",
    "    \n",
    "    words = [word\n",
    "             for discussion in stemmed_samples\n",
    "             for utterance in discussion\n",
    "             for text in (utterance[0], utterance[1])\n",
    "             for word in text]\n",
    "    # Fit the tokenizer on train texts\n",
    "    word_index, word_counts = np.unique(words, return_counts=True)\n",
    "    new_indices = sorted(range(len(word_index)), key=lambda i: \"0\"+word_index[i] if word_index[i] in people_main else \"1\"+word_index[i])\n",
    "    word_index = word_index[new_indices]\n",
    "    word_counts = word_counts[new_indices]\n",
    "    \n",
    "    inverse_words = {v: i+2 for i, v in enumerate(word_index)}\n",
    "    inverse_people = {v: i for i, v in enumerate(people_main)}\n",
    "\n",
    "    # Convert them to indices and truncate them if they are too large\n",
    "    tokenized_samples = [\n",
    "        # Pad the discussion so that its length matches `discussion_length`\n",
    "        pad([(pad([inverse_words.get(w, 1) for w in utterance[0]], utterance_length, 0),\n",
    "              pad([inverse_words.get(w, 1) for w in utterance[1]], narration_length, 0),\n",
    "              [inverse_people.get(p, 0) for p in utterance[2]],\n",
    "              [inverse_people.get(p, 0) for p in utterance[3]],\n",
    "              inverse_people.get(utterance[4], -1)+1)\n",
    "              for utterance in discussion if utterance[4] in inverse_people],\n",
    "             length=discussion_length,\n",
    "             default=([0]*(utterance_length or 0), # empty utterance\n",
    "                      [0]*(narration_length or 0), # empty narration,\n",
    "                      [], # no target hint\n",
    "                      [], # no target hint\n",
    "                      0, # default non-character id\n",
    "             ))\n",
    "        for discussion in (tqdm(stemmed_samples, desc=\"Text/targets to ids mapping\")\n",
    "                           if verbose > 1 else stemmed_samples)]\n",
    "\n",
    "    return tokenized_samples, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text words stemming: 100%|██████████| 94/94 [00:02<00:00, 42.43it/s]\n",
      "Text/targets to ids mapping: 100%|██████████| 94/94 [00:00<00:00, 683.54it/s]\n"
     ]
    }
   ],
   "source": [
    "res = make_data(dataset, people,\n",
    "                utterance_length=params[\"UTTERANCE_LENGTH\"],\n",
    "                narration_length=params[\"NARRATION_LENGTH\"],\n",
    "                discussion_length=params[\"DISCUSSION_LENGTH\"])\n",
    "processed_dataset, word_index = res\n",
    "#res[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape the dataset as matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrices(data, discussion_length, voc_size, people_count, target_count):\n",
    "    \"\"\"Transform a list of samples into a tuple of matrices to feed into the model\"\"\"\n",
    "    utterance_matrices = np.zeros((len(data), discussion_length, voc_size)) # Set of words -> Bag of words one-hot encoding\n",
    "    narration_matrices = np.zeros((len(data), discussion_length, voc_size)) # Set of words -> Bag of words one-hot encoding\n",
    "    speaker_matrices = np.zeros((len(data), discussion_length, people_count)) # Set of people -> Bag of words one-hot encoding\n",
    "    dest_matrices = np.zeros((len(data), discussion_length, people_count)) # Set of people -> Bag of words one-hot encoding\n",
    "    target_matrices = np.zeros((len(data), discussion_length, target_count)) # Categorical target -> One-hot encoding\n",
    "    for discussion_i, discussion in enumerate(data):\n",
    "        for utterance_i, utterance in enumerate(discussion):\n",
    "            utterance_matrices[discussion_i, utterance_i, utterance[0]] = 1\n",
    "            narration_matrices[discussion_i, utterance_i, utterance[1]] = 1\n",
    "            speaker_matrices[discussion_i, utterance_i, list(utterance[2])] = 1\n",
    "            dest_matrices[discussion_i, utterance_i, list(utterance[3])] = 1\n",
    "            target_matrices[discussion_i, utterance_i, utterance[4]] = 1\n",
    "    \n",
    "    return utterance_matrices, narration_matrices, speaker_matrices, dest_matrices, target_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_matrices, narration_matrices, speaker_hint_matrices, dest_hint_matrices, target_matrices = \\\n",
    "    make_matrices(processed_dataset,\n",
    "              discussion_length=params['DISCUSSION_LENGTH'],\n",
    "              voc_size=len(word_index)+2,\n",
    "              people_count=len(people),\n",
    "              target_count=len(people)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/validation sets & sample weighting\n",
    "\n",
    "Because we have imbalanced classes and padding utterances, we need to weights them to correct the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_utterance_matrices,   test_utterance_matrices,\n",
    " train_narration_matrices,   test_narration_matrices,\n",
    " train_speaker_hint_matrices, test_speaker_hint_matrices,\n",
    " train_dest_hint_matrices, test_dest_hint_matrices,\n",
    " train_target_matrices, test_target_matrices) = \\\n",
    "    train_test_split(utterance_matrices, narration_matrices, speaker_hint_matrices, dest_hint_matrices, target_matrices,\n",
    "                     test_size=params['TEST_SPLIT'])\n",
    "    \n",
    "(train_utterance_matrices,   val_utterance_matrices,\n",
    "train_narration_matrices,   val_narration_matrices,\n",
    "train_speaker_hint_matrices, val_speaker_hint_matrices,\n",
    "train_dest_hint_matrices, val_dest_hint_matrices,\n",
    "train_target_matrices, val_target_matrices) = \\\n",
    "train_test_split(train_utterance_matrices, train_narration_matrices, train_speaker_hint_matrices, train_dest_hint_matrices, train_target_matrices,\n",
    "                 test_size=params['VALIDATION_SPLIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the classes\n",
    "targets_set, targets_inverse, targets_count = np.unique(np.argwhere(train_target_matrices)[:, -1], return_inverse=True, return_counts=True)\n",
    "\n",
    "# And transform these counts into samples\n",
    "sample_weight = np.zeros_like(targets_inverse, dtype=float)\n",
    "sample_weight[targets_inverse == 0] = 0.00\n",
    "total = sum(targets_count) - targets_count[0]\n",
    "for target_id, target_count in zip(targets_set, targets_count):\n",
    "    sample_weight[targets_inverse == target_id] = target_count/total\n",
    "sample_weight = sample_weight.reshape(train_target_matrices.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid zero-sum sample weights\n",
    "#sample_weight[np.argwhere(sample_weight.sum(axis=1) == 0).reshape(-1)] += 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_input_layer = keras.layers.Input(shape=(None, len(word_index)+2)) # leave the dataset length that will be batched\n",
    "narrations_input_layer = keras.layers.Input(shape=(None, len(word_index)+2)) # leave the dataset length that will be batched\n",
    "speaker_hint_input_layer = keras.layers.Input(shape=(None, len(people))) # leave the dataset length that will be batched\n",
    "dest_hint_input_layer = keras.layers.Input(shape=(None, len(people))) # leave the dataset length that will be batched\n",
    "\n",
    "#embedding_layer = keras.layers.Embedding(len(word_index), params[\"WORD_EMBEDDING_DIM\"], name=\"word_embedding\")\n",
    "word_bag_layer = keras.layers.Dense(params[\"WORD_EMBEDDING_DIM\"])#, kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "people_hint_bag_layer = keras.layers.Dense(params[\"PEOPLE_EMBEDDING_DIM\"])\n",
    "                                           #kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "#dest_hint_bag_layer = keras.layers.Dense(params[\"PEOPLE_EMBEDDING_DIM\"])\n",
    "concat_layer = keras.layers.Concatenate(name=\"lstm_input\")\n",
    "\n",
    "concat_input = concat_layer([\n",
    "    word_bag_layer(utterances_input_layer),\n",
    "    word_bag_layer(narrations_input_layer),\n",
    "    people_hint_bag_layer(speaker_hint_input_layer),\n",
    "    people_hint_bag_layer(dest_hint_input_layer),\n",
    "])\n",
    "lstm_input = concat_input#keras.layers.Activation('relu')(concat_input)\n",
    "\n",
    "# not mandatory to set the LSTM dim output to the people hints dim input but it seems more coherent\n",
    "\n",
    "lstm_layer = keras.layers.GRU(params['RECURRENT_UNITS_COUNT'], return_sequences=True, dropout=0.2)\n",
    "lstm_output = lstm_layer(lstm_input)\n",
    "\n",
    "output_layer = keras.layers.TimeDistributed(keras.layers.Dense(len(people)+1, activation='softmax'))\n",
    "#lstm_layer = keras.layers.Dense(len(people)+1, activation='softmax')\n",
    "output = output_layer(lstm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs = [utterances_input_layer, narrations_input_layer, speaker_hint_input_layer, dest_hint_input_layer] , outputs = [output])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'],\n",
    "              sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run   0 (~    50 epochs) train accuracy: 0.29237, val accuracy: 0.048711\n",
      "run   1 (~   100 epochs) train accuracy: 0.29379, val accuracy: 0.048711\n",
      "run   2 (~   150 epochs) train accuracy: 0.29944, val accuracy: 0.054441\n",
      "run   3 (~   200 epochs) train accuracy: 0.30932, val accuracy: 0.057307\n",
      "run   4 (~   250 epochs) train accuracy: 0.31356, val accuracy: 0.060172\n",
      "run   5 (~   300 epochs) train accuracy: 0.31497, val accuracy: 0.077364\n",
      "run   6 (~   350 epochs) train accuracy: 0.32345, val accuracy: 0.074499\n",
      "run   7 (~   400 epochs) train accuracy: 0.32768, val accuracy: 0.068768\n",
      "run   8 (~   450 epochs) train accuracy: 0.34181, val accuracy: 0.071633\n",
      "run   9 (~   500 epochs) train accuracy: 0.34463, val accuracy: 0.077364\n",
      "run  10 (~   550 epochs) train accuracy: 0.35028, val accuracy: 0.080229\n",
      "run  11 (~   600 epochs) train accuracy: 0.35593, val accuracy: 0.080229\n",
      "run  12 (~   650 epochs) train accuracy: 0.36723, val accuracy: 0.080229\n",
      "run  13 (~   700 epochs) train accuracy: 0.36864, val accuracy: 0.077364\n",
      "run  14 (~   750 epochs) train accuracy: 0.38559, val accuracy: 0.08596\n",
      "run  15 (~   800 epochs) train accuracy: 0.39407, val accuracy: 0.10602\n",
      "run  16 (~   850 epochs) train accuracy: 0.39831, val accuracy: 0.10029\n",
      "run  17 (~   900 epochs) train accuracy: 0.40960, val accuracy: 0.094556\n"
     ]
    }
   ],
   "source": [
    "epochs_count = 50\n",
    "try:\n",
    "    history = []\n",
    "    for run_i in range(20):\n",
    "        model.fit(x=[train_utterance_matrices, train_narration_matrices, train_speaker_hint_matrices, train_dest_hint_matrices],\n",
    "                  y=[train_target_matrices],\n",
    "                  epochs=epochs_count,\n",
    "                  sample_weight=sample_weight,\n",
    "                  verbose=0, batch_size=params['BATCH_SIZE'])\n",
    "\n",
    "        target_argmax = val_target_matrices.argmax(axis=2).reshape(-1)\n",
    "        val_accuracy = accuracy_score(\n",
    "            model.predict(x=[val_utterance_matrices, val_narration_matrices, val_speaker_hint_matrices, val_dest_hint_matrices]).argmax(axis=2).reshape(-1),\n",
    "            val_target_matrices.argmax(axis=2).reshape(-1),\n",
    "            sample_weight=(target_argmax != 0)\n",
    "        )\n",
    "\n",
    "        target_argmax = train_target_matrices.argmax(axis=2).reshape(-1)\n",
    "        train_accuracy = accuracy_score(\n",
    "            model.predict(x=[train_utterance_matrices, train_narration_matrices, train_speaker_hint_matrices, train_dest_hint_matrices]).argmax(axis=2).reshape(-1),\n",
    "            train_target_matrices.argmax(axis=2).reshape(-1),\n",
    "            sample_weight=(target_argmax != 0)\n",
    "        )\n",
    "        history.append((run_i, train_accuracy, val_accuracy))\n",
    "        print(\"run {: 3d} (~ {: 5d} epochs) train accuracy: {:.5f}, val accuracy: {:.5}\".format(run_i, (run_i+1)*epochs_count, train_accuracy, val_accuracy))\n",
    "except KeyboardInterrupt as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runs ids above are offseted by 20, ie the model has been fitted on 1000 more epochs before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15767634854771784"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target_argmax = test_target_matrices.argmax(axis=2).reshape(-1)\n",
    "accuracy_score(\n",
    "    model.predict(x=[test_utterance_matrices, test_narration_matrices, test_speaker_hint_matrices, test_dest_hint_matrices]).argmax(axis=2).reshape(-1),\n",
    "    test_target_argmax,\n",
    "    sample_weight=(test_target_argmax != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code was used to help us quicly choose a structure that matched the properties\n",
    "we wanted the model to have   \n",
    "(alternating speakers and transmission of hints to the output)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design fake data to force the model to pay attention to the hints\n",
    "\n",
    "We use this data to evaluate the performance of a structure according to properties we know it should have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mono_discussion_count = len(people)#+1\n",
    "fake_utterance_length = 40\n",
    "fake_mono_utterance_matrices = np.zeros((fake_mono_discussion_count, 1, len(word_index)+2))\n",
    "fake_mono_narration_matrices = np.zeros((fake_mono_discussion_count, 1, len(word_index)+2))\n",
    "\n",
    "fake_mono_people_hint_matrices = np.zeros((fake_mono_discussion_count, 1, len(people)))\n",
    "fake_mono_people_hint_matrices[np.arange(len(people)), 0, np.arange(len(people))] = 1\n",
    "fake_mono_target_matrices = np.zeros((fake_mono_discussion_count, 1, len(people)+1))\n",
    "fake_mono_target_matrices[np.arange(len(people)), 0, np.arange(len(people))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mono_utterance_matrices = fake_mono_utterance_matrices.repeat(50, axis=0)\n",
    "fake_mono_narration_matrices = fake_mono_narration_matrices.repeat(50, axis=0)\n",
    "fake_mono_people_hint_matrices = fake_mono_people_hint_matrices.repeat(50, axis=0)\n",
    "fake_mono_target_matrices = fake_mono_target_matrices.repeat(50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(fake_mono_utterance_matrices.shape[0]):\n",
    "    for j in range(fake_mono_utterance_matrices.shape[1]):\n",
    "        fake_mono_utterance_matrices[i][j][np.random.choice(fake_mono_utterance_matrices.shape[2], fake_utterance_length)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 2s 689us/step - loss: 3.3883 - acc: 0.2356\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 2s 652us/step - loss: 3.3707 - acc: 0.2374\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 2s 736us/step - loss: 3.3606 - acc: 0.2437\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 2s 662us/step - loss: 3.3515 - acc: 0.2556\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 2s 702us/step - loss: 3.3436 - acc: 0.2652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a3e6e80>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=[fake_mono_utterance_matrices, fake_mono_people_hint_matrices],\n",
    "    y=[fake_mono_target_matrices],\n",
    "    verbose=1,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 1s 198us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4065698538886176, 1.0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(fake_mono_target_matrices[[0, 50]])\n",
    "model.evaluate([fake_mono_utterance_matrices, fake_mono_people_hint_matrices], [fake_mono_target_matrices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design fake data to force the model to pay attention to alternative speakers\n",
    "\n",
    "We use this data to evaluate the performance of a structure according to properties we know it should have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_people_pairs = np.array([np.random.choice(len(people), 2) for _ in range(500)])\n",
    "\n",
    "fake_duo_discussion_count = len(fake_people_pairs)\n",
    "fake_duo_discussion_length = 32\n",
    "\n",
    "fake_duo_utterance_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(word_index)+2))\n",
    "fake_duo_narration_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(word_index)+2))\n",
    "\n",
    "fake_duo_people_hint_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(people)))\n",
    "# get a hint in the first utterance about the first speaker\n",
    "fake_duo_people_hint_matrices[np.arange(fake_duo_discussion_count), 0, fake_people_pairs[:, 0]] = 1\n",
    "# get a hint in the second utterance about the second speaker\n",
    "fake_duo_people_hint_matrices[np.arange(fake_duo_discussion_count), 1, fake_people_pairs[:, 1]] = 1\n",
    "\n",
    "for i in range(fake_duo_utterance_matrices.shape[0]):\n",
    "    for j in range(fake_duo_utterance_matrices.shape[1]):\n",
    "        fake_duo_utterance_matrices[i][j][np.random.choice(fake_duo_utterance_matrices.shape[2], fake_utterance_length)] = 1\n",
    "        \n",
    "fake_duo_target_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(people)+1))\n",
    "for i in range(fake_duo_discussion_length//2):\n",
    "    fake_duo_target_matrices[np.arange(fake_duo_discussion_count), i*2, fake_people_pairs[:, 0]] = 1\n",
    "    fake_duo_target_matrices[np.arange(fake_duo_discussion_count), i*2+1, fake_people_pairs[:, 1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(fake_mono_target_matrices[[0, 50]])\n",
    "model.evaluate([fake_duo_utterance_matrices, fake_duo_people_hint_matrices], [fake_duo_target_matrices])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
