{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perceval/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/perceval/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=100, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"UTTERANCE_LENGTH\": 60,\n",
    "    \"DISCUSSION_LENGTH\": 48, #60,\n",
    "    \"NARRATION_LENGTH\": 30,\n",
    "    \"MAX_WORDS_COUNT\": 3000,\n",
    "    \"WORD_EMBEDDING_DIM\": 32,\n",
    "    \"PEOPLE_EMBEDDING_DIM\": 32,\n",
    "    \"VALIDATION_SPLIT\": 0.2,\n",
    "    \"LSTM_UNITS_COUNT\": 64,\n",
    "    \"PATIENCE\": 2,\n",
    "    \"EPOCHS\": 8,\n",
    "    \"BATCH_SIZE\": 16, # not large because every sample is 300 words long\n",
    "    \"LR\": 0.001,\n",
    "    \"MSE_LOSS_WEIGHT\": 500,\n",
    "}\n",
    "\n",
    "\n",
    "TIME_STR = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_MODEL_PATH = os.path.join('./output/', \"model-{}.h5\".format(TIME_STR))\n",
    "CHECKPOINT_PATH = os.path.join('./output/', \"model-{}-checkpoint.h5\".format(TIME_STR))\n",
    "GLOVE_PATH = '/Users/perceval/Developpement/Data/glove.6B.100d/glove.6B.100d.txt'\n",
    "DATA_PATH = 'corpus/dataset_dl.pkl'\n",
    "SPLIT_REGEX = '[^a-zA-Z](‘)|(‘)[^a-zA-Z]|(?:\\s|\\n)+|(\\d+\\.\\d*)|([\".!,;:-])'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pickle.load(open(\"corpus/people.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"corpus/dataset-prolog.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My dear Mr_Bennet,  have you heard that Netherfield Park is let at last?',\n",
       " ' said his lady to him one day, ',\n",
       " {'Charlotte', 'Mrs_Bennet'},\n",
       " 'Mrs_Bennet',\n",
       " 'wife',\n",
       " 'F',\n",
       " None)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and process it (again !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to stem the data, to make the vocabulary denser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentence(stemmer, sentence, people_names):\n",
    "    tokens = re.findall(r\"[@_\\w]+|['.,!?;]\", sentence)\n",
    "    return [stemmer.stem(word) if word not in people_names else word for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr_Bennet', 'is', 'with', 'Mrs_Bennet', \"'\", 's', 'daughter']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_sentence(SnowballStemmer('english'), \"Mr_Bennet is with Mrs_Bennet's daughter\", [p['main'] for p in people])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(list, length, default):\n",
    "    if length is None:\n",
    "        return list\n",
    "    return  [list[i] if i < len(list) else default for i in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, people, max_count=3000, verbose=2, utterance_length=8, narration_length=None, discussion_length=None):\n",
    "    people_main = [p['main'] for p in people]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    stemmed_samples = [\n",
    "        [(stem_sentence(stemmer, utterance[0], people_main),\n",
    "          stem_sentence(stemmer, utterance[1], people_main),\n",
    "          utterance[2],\n",
    "          utterance[3])\n",
    "         for utterance in discussion]\n",
    "        for discussion in (tqdm(dataset, desc=\"Text words stemming\")\n",
    "                           if verbose > 1 else dataset)]\n",
    "    \n",
    "    words = [word\n",
    "             for discussion in stemmed_samples\n",
    "             for utterance in discussion\n",
    "             for text in (utterance[0], utterance[1])\n",
    "             for word in text]\n",
    "    # Fit the tokenizer on train texts\n",
    "    word_index, word_counts = np.unique(words, return_counts=True)\n",
    "    new_indices = sorted(range(len(word_index)), key=lambda i: \"0\"+word_index[i] if word_index[i] in people_main else \"1\"+word_index[i])\n",
    "    word_index = word_index[new_indices]\n",
    "    word_counts = word_counts[new_indices]\n",
    "    \n",
    "    inverse_words = {v: i+2 for i, v in enumerate(word_index)}\n",
    "    inverse_people = {v: i for i, v in enumerate(people_main)}\n",
    "\n",
    "    # Convert them to indices and truncate them if they are too large\n",
    "    tokenized_samples = [\n",
    "        # Pad the discussion so that its length matches `discussion_length`\n",
    "        pad([(pad([inverse_words.get(w, 1) for w in utterance[0]], utterance_length, 0),\n",
    "              pad([inverse_words.get(w, 1) for w in utterance[1]], narration_length, 0),\n",
    "              [inverse_people.get(p, 0) for p in utterance[2]],\n",
    "              inverse_people.get(utterance[3], -1)+1)\n",
    "              for utterance in discussion],\n",
    "             length=discussion_length,\n",
    "             default=([0]*(utterance_length or 0), # empty utterance\n",
    "                      [0]*(narration_length or 0), # empty narration,\n",
    "                      [], # no target hint\n",
    "                      0, # default non-character id\n",
    "             ))\n",
    "        for discussion in (tqdm(stemmed_samples, desc=\"Text/targets to ids mapping\")\n",
    "                           if verbose > 1 else stemmed_samples)]\n",
    "\n",
    "    return tokenized_samples, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text words stemming: 100%|██████████| 94/94 [00:02<00:00, 41.23it/s]\n",
      "Text/targets to ids mapping: 100%|██████████| 94/94 [00:00<00:00, 683.33it/s]\n"
     ]
    }
   ],
   "source": [
    "res = make_data(dataset, people,\n",
    "                max_count=params[\"MAX_WORDS_COUNT\"],\n",
    "                utterance_length=params[\"UTTERANCE_LENGTH\"],\n",
    "                narration_length=params[\"NARRATION_LENGTH\"],\n",
    "                discussion_length=params[\"DISCUSSION_LENGTH\"])\n",
    "processed_dataset, word_index = res\n",
    "#res[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape the dataset as matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrices(data, discussion_length, voc_size, people_count, target_count):\n",
    "    utterance_matrices = np.zeros((len(data), discussion_length, voc_size)) # Set of words -> Bag of words one-hot encoding\n",
    "    narration_matrices = np.zeros((len(data), discussion_length, voc_size)) # Set of words -> Bag of words one-hot encoding\n",
    "    people_matrices = np.zeros((len(data), discussion_length, people_count)) # Set of people -> Bag of words one-hot encoding\n",
    "    target_matrices = np.zeros((len(data), discussion_length, target_count)) # Categorical target -> One-hot encoding\n",
    "    for discussion_i, discussion in enumerate(data):\n",
    "        for utterance_i, utterance in enumerate(discussion):\n",
    "            utterance_matrices[discussion_i, utterance_i, utterance[0]] = 1\n",
    "            narration_matrices[discussion_i, utterance_i, utterance[1]] = 1\n",
    "            people_matrices[discussion_i, utterance_i, list(utterance[2])] = 1\n",
    "            target_matrices[discussion_i, utterance_i, utterance[3]] = 1\n",
    "    \n",
    "    return utterance_matrices, narration_matrices, people_matrices, target_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_matrices, narration_matrices, people_hint_matrices, target_matrices = \\\n",
    "    make_matrices(processed_dataset,\n",
    "              discussion_length=params['DISCUSSION_LENGTH'],\n",
    "              voc_size=len(word_index)+2,\n",
    "              people_count=len(people),\n",
    "              target_count=len(people)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like now ? let's look at the first utterance of the first discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dataset[0][0])\n",
    "# display(processed_dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_input_layer = keras.layers.Input(shape=(None, len(word_index)+2)) # leave the dataset length that will be batched\n",
    "narrations_input_layer = keras.layers.Input(shape=(None, len(word_index)+2)) # leave the dataset length that will be batched\n",
    "people_hint_input_layer = keras.layers.Input(shape=(None, len(people))) # leave the dataset length that will be batched\n",
    "\n",
    "#embedding_layer = keras.layers.Embedding(len(word_index), params[\"WORD_EMBEDDING_DIM\"], name=\"word_embedding\")\n",
    "word_bag_layer = keras.layers.Dense(params[\"WORD_EMBEDDING_DIM\"])#, kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "people_hint_bag_layer = keras.layers.Dense(params[\"PEOPLE_EMBEDDING_DIM\"])\n",
    "concat_layer = keras.layers.Concatenate(name=\"lstm_input\")\n",
    "\n",
    "concat_input = concat_layer([\n",
    "    word_bag_layer(utterances_input_layer),\n",
    "    word_bag_layer(narrations_input_layer),\n",
    "    people_hint_input_layer, #people_hint_bag_layer(people_hint_input_layer),#\n",
    "])\n",
    "lstm_input = concat_input#keras.layers.Activation('relu')(concat_input)\n",
    "\n",
    "# not mandatory to set the LSTM dim output to the people hints dim input but it seems more coherent\n",
    "\n",
    "lstm_layer = keras.layers.GRU(params['LSTM_UNITS_COUNT'], return_sequences=True)\n",
    "lstm_output = dropout(lstm_layer(lstm_input))\n",
    "\n",
    "output_layer = keras.layers.TimeDistributed(keras.layers.Dense(len(people)+1, activation='softmax'))\n",
    "#lstm_layer = keras.layers.Dense(len(people)+1, activation='softmax')\n",
    "output = output_layer(lstm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs = [utterances_input_layer, narrations_input_layer, people_hint_input_layer] , outputs = [output])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'],\n",
    "              sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weighting\n",
    "\n",
    "Because we have imbalanced classes and padding utterances, we need to weights them to correct the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_utterance_matrices,   test_utterance_matrices,\n",
    " train_narration_matrices,   test_narration_matrices,\n",
    " train_people_hint_matrices, test_people_hint_matrices,\n",
    " train_target_matrices, test_target_matrices) = \\\n",
    "    train_test_split(utterance_matrices, narration_matrices, people_hint_matrices, target_matrices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the classes\n",
    "targets_set, targets_inverse, targets_count = np.unique(np.argwhere(train_target_matrices)[:, -1], return_inverse=True, return_counts=True)\n",
    "\n",
    "# And transform these counts into samples\n",
    "sample_weight = np.zeros_like(targets_inverse, dtype=float)\n",
    "sample_weight[targets_inverse == 0] = 0.00\n",
    "total = sum(targets_count) - targets_count[0]\n",
    "for target_id, target_count in zip(targets_set, targets_count):\n",
    "    sample_weight[targets_inverse == target_id] = target_count/total\n",
    "sample_weight = sample_weight.reshape(train_target_matrices.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight[np.argwhere(sample_weight.sum(axis=1) == 0).reshape(-1)] += 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 7s 121ms/step - loss: 8.5210 - acc: 0.5510 - val_loss: 7.9873 - val_acc: 0.6556\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 7.5319 - acc: 0.7434 - val_loss: 6.0903 - val_acc: 0.6556\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 19ms/step - loss: 4.2999 - acc: 0.7434 - val_loss: 0.8290 - val_acc: 0.6556\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.3712 - acc: 0.7434 - val_loss: 0.1194 - val_acc: 0.6556\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1515 - acc: 0.7434 - val_loss: 0.1116 - val_acc: 0.6556\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1391 - acc: 0.7434 - val_loss: 0.1086 - val_acc: 0.6556\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1330 - acc: 0.7434 - val_loss: 0.1041 - val_acc: 0.6556\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1229 - acc: 0.7434 - val_loss: 0.0991 - val_acc: 0.6556\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1197 - acc: 0.7434 - val_loss: 0.0945 - val_acc: 0.6556\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1132 - acc: 0.7434 - val_loss: 0.0916 - val_acc: 0.6556\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1074 - acc: 0.7434 - val_loss: 0.0885 - val_acc: 0.6556\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1056 - acc: 0.7434 - val_loss: 0.0853 - val_acc: 0.6556\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0997 - acc: 0.7434 - val_loss: 0.0822 - val_acc: 0.6556\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0935 - acc: 0.7434 - val_loss: 0.0780 - val_acc: 0.6556\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0898 - acc: 0.7434 - val_loss: 0.0754 - val_acc: 0.6556\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0871 - acc: 0.7434 - val_loss: 0.0723 - val_acc: 0.6556\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0809 - acc: 0.7434 - val_loss: 0.0693 - val_acc: 0.6556\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0772 - acc: 0.7434 - val_loss: 0.0659 - val_acc: 0.6556\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0730 - acc: 0.7438 - val_loss: 0.0642 - val_acc: 0.6556\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0690 - acc: 0.7437 - val_loss: 0.0623 - val_acc: 0.6556\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0661 - acc: 0.7438 - val_loss: 0.0601 - val_acc: 0.6556\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0627 - acc: 0.7441 - val_loss: 0.0580 - val_acc: 0.6556\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0597 - acc: 0.7444 - val_loss: 0.0560 - val_acc: 0.6556\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0589 - acc: 0.7438 - val_loss: 0.0553 - val_acc: 0.6556\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0558 - acc: 0.7444 - val_loss: 0.0543 - val_acc: 0.6556\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0537 - acc: 0.7441 - val_loss: 0.0525 - val_acc: 0.6556\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0513 - acc: 0.7458 - val_loss: 0.0514 - val_acc: 0.6556\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0505 - acc: 0.7451 - val_loss: 0.0507 - val_acc: 0.6556\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0494 - acc: 0.7455 - val_loss: 0.0497 - val_acc: 0.6556\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0471 - acc: 0.7462 - val_loss: 0.0493 - val_acc: 0.6556\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0451 - acc: 0.7469 - val_loss: 0.0486 - val_acc: 0.6556\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.0460 - acc: 0.7500 - val_loss: 0.0479 - val_acc: 0.6556\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0442 - acc: 0.7490 - val_loss: 0.0473 - val_acc: 0.6556\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0423 - acc: 0.7524 - val_loss: 0.0468 - val_acc: 0.6569\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.0410 - acc: 0.7510 - val_loss: 0.0463 - val_acc: 0.6583\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.0399 - acc: 0.7538 - val_loss: 0.0460 - val_acc: 0.6625\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - acc: 0.7542 - val_loss: 0.0456 - val_acc: 0.6639\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0389 - acc: 0.7556 - val_loss: 0.0451 - val_acc: 0.6625\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0386 - acc: 0.7549 - val_loss: 0.0450 - val_acc: 0.6639\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.0374 - acc: 0.7542 - val_loss: 0.0448 - val_acc: 0.6639\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.0360 - acc: 0.7580 - val_loss: 0.0446 - val_acc: 0.6653\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.0364 - acc: 0.7576 - val_loss: 0.0443 - val_acc: 0.6653\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.0354 - acc: 0.7590 - val_loss: 0.0442 - val_acc: 0.6653\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.0344 - acc: 0.7608 - val_loss: 0.0441 - val_acc: 0.6653\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.0342 - acc: 0.7625 - val_loss: 0.0440 - val_acc: 0.6653\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0344 - acc: 0.7618 - val_loss: 0.0437 - val_acc: 0.6653\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0335 - acc: 0.7625 - val_loss: 0.0434 - val_acc: 0.6653\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0332 - acc: 0.7639 - val_loss: 0.0434 - val_acc: 0.6653\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0334 - acc: 0.7628 - val_loss: 0.0432 - val_acc: 0.6653\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0319 - acc: 0.7615 - val_loss: 0.0429 - val_acc: 0.6653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17785cc50>"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_utterance_matrices, train_narration_matrices, train_people_hint_matrices],\n",
    "          y=[train_target_matrices],\n",
    "          epochs=50,\n",
    "          sample_weight=sample_weight,\n",
    "          validation_split=0.2,\n",
    "          verbose=1, batch_size=8)\n",
    "    # print('-----')\n",
    "    # print(model.evaluate([train_utterance_matrices, train_narration_matrices, train_people_hint_matrices], train_targets_matrices))    \n",
    "    # print(model.evaluate([test_utterance_matrices, test_narration_matrices, test_people_hint_matrices], test_targets_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 340us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.182927131652832, 0.21052631735801697]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(\n",
    "    x=[test_utterance_matrices[:, :1, :], test_narration_matrices[:, :1, :], test_people_hint_matrices[:, :1, :]],\n",
    "    y=[test_target_matrices[:, :1, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 55)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=[test_utterance_matrices, test_narration_matrices, test_people_hint_matrices]).reshape(-1, test_target_matrices.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912,)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(test_target_matrices.reshape(-1, test_target_matrices.shape[-1]))[:, 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-382-6c321c05f216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m accuracy_score(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_utterance_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_narration_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_people_hint_matrices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_target_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_matrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    model.predict(x=[test_utterance_matrices, test_narration_matrices, test_people_hint_matrices]).reshape(-1, test_target_matrices.shape[-1]),\n",
    "    test_target_matrices.reshape(-1, test_target_matrices.shape[-1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0864 - acc: 0.3604\n",
      "Epoch 2/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0875 - acc: 0.3484\n",
      "Epoch 3/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0863 - acc: 0.3644\n",
      "Epoch 4/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0901 - acc: 0.3457\n",
      "Epoch 5/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0851 - acc: 0.3511\n",
      "Epoch 6/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0844 - acc: 0.3497\n",
      "Epoch 7/150\n",
      "94/94 [==============================] - 0s 961us/step - loss: 0.0850 - acc: 0.3577\n",
      "Epoch 8/150\n",
      "94/94 [==============================] - 0s 943us/step - loss: 0.0864 - acc: 0.3537\n",
      "Epoch 9/150\n",
      "94/94 [==============================] - 0s 963us/step - loss: 0.0875 - acc: 0.3391\n",
      "Epoch 10/150\n",
      "94/94 [==============================] - 0s 946us/step - loss: 0.0874 - acc: 0.3457\n",
      "Epoch 11/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0834 - acc: 0.3630\n",
      "Epoch 12/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0873 - acc: 0.3577\n",
      "Epoch 13/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0834 - acc: 0.3564\n",
      "Epoch 14/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0836 - acc: 0.3564\n",
      "Epoch 15/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0811 - acc: 0.3537\n",
      "Epoch 16/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0823 - acc: 0.3590\n",
      "Epoch 17/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0860 - acc: 0.3551\n",
      "Epoch 18/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0818 - acc: 0.3577\n",
      "Epoch 19/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0854 - acc: 0.3497\n",
      "Epoch 20/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0884 - acc: 0.3564\n",
      "Epoch 21/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0801 - acc: 0.3657\n",
      "Epoch 22/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0854 - acc: 0.3511\n",
      "Epoch 23/150\n",
      "94/94 [==============================] - 0s 999us/step - loss: 0.0815 - acc: 0.3497\n",
      "Epoch 24/150\n",
      "94/94 [==============================] - 0s 980us/step - loss: 0.0821 - acc: 0.3564\n",
      "Epoch 25/150\n",
      "94/94 [==============================] - 0s 945us/step - loss: 0.0813 - acc: 0.3524\n",
      "Epoch 26/150\n",
      "94/94 [==============================] - 0s 984us/step - loss: 0.0844 - acc: 0.3457\n",
      "Epoch 27/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0826 - acc: 0.3590\n",
      "Epoch 28/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0846 - acc: 0.3497\n",
      "Epoch 29/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0814 - acc: 0.3551\n",
      "Epoch 30/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0791 - acc: 0.3564\n",
      "Epoch 31/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0821 - acc: 0.3617\n",
      "Epoch 32/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0813 - acc: 0.3577\n",
      "Epoch 33/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0802 - acc: 0.3471\n",
      "Epoch 34/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0785 - acc: 0.3524\n",
      "Epoch 35/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0775 - acc: 0.3630\n",
      "Epoch 36/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0816 - acc: 0.3590\n",
      "Epoch 37/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0793 - acc: 0.3617\n",
      "Epoch 38/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0773 - acc: 0.3551\n",
      "Epoch 39/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0836 - acc: 0.3471\n",
      "Epoch 40/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0779 - acc: 0.3590\n",
      "Epoch 41/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0784 - acc: 0.3551\n",
      "Epoch 42/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0821 - acc: 0.3604\n",
      "Epoch 43/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0769 - acc: 0.3617\n",
      "Epoch 44/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0820 - acc: 0.3524\n",
      "Epoch 45/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0772 - acc: 0.3551\n",
      "Epoch 46/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0782 - acc: 0.3617\n",
      "Epoch 47/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0772 - acc: 0.3630\n",
      "Epoch 48/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0749 - acc: 0.3590\n",
      "Epoch 49/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0754 - acc: 0.3537\n",
      "Epoch 50/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0785 - acc: 0.3537\n",
      "Epoch 51/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0815 - acc: 0.3497\n",
      "Epoch 52/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0743 - acc: 0.3670\n",
      "Epoch 53/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0754 - acc: 0.3657\n",
      "Epoch 54/150\n",
      "94/94 [==============================] - 0s 992us/step - loss: 0.0783 - acc: 0.3590\n",
      "Epoch 55/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0756 - acc: 0.3577\n",
      "Epoch 56/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0760 - acc: 0.3670\n",
      "Epoch 57/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0777 - acc: 0.3537\n",
      "Epoch 58/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0733 - acc: 0.3630\n",
      "Epoch 59/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0750 - acc: 0.3630\n",
      "Epoch 60/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0758 - acc: 0.3670\n",
      "Epoch 61/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0770 - acc: 0.3444\n",
      "Epoch 62/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0763 - acc: 0.3590\n",
      "Epoch 63/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0746 - acc: 0.3604\n",
      "Epoch 64/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0707 - acc: 0.3803\n",
      "Epoch 65/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0767 - acc: 0.3497\n",
      "Epoch 66/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0737 - acc: 0.3564\n",
      "Epoch 67/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0742 - acc: 0.3644\n",
      "Epoch 68/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0753 - acc: 0.3644\n",
      "Epoch 69/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0728 - acc: 0.3697\n",
      "Epoch 70/150\n",
      "94/94 [==============================] - 0s 996us/step - loss: 0.0724 - acc: 0.3684\n",
      "Epoch 71/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0715 - acc: 0.3617\n",
      "Epoch 72/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0733 - acc: 0.3617\n",
      "Epoch 73/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0751 - acc: 0.3630\n",
      "Epoch 74/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0711 - acc: 0.3670\n",
      "Epoch 75/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0731 - acc: 0.3710\n",
      "Epoch 76/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0727 - acc: 0.3710\n",
      "Epoch 77/150\n",
      "94/94 [==============================] - 0s 935us/step - loss: 0.0699 - acc: 0.3697\n",
      "Epoch 78/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0715 - acc: 0.3670\n",
      "Epoch 79/150\n",
      "94/94 [==============================] - 0s 895us/step - loss: 0.0746 - acc: 0.3577\n",
      "Epoch 80/150\n",
      "94/94 [==============================] - 0s 901us/step - loss: 0.0702 - acc: 0.3604\n",
      "Epoch 81/150\n",
      "94/94 [==============================] - 0s 898us/step - loss: 0.0685 - acc: 0.3870\n",
      "Epoch 82/150\n",
      "94/94 [==============================] - 0s 909us/step - loss: 0.0699 - acc: 0.3657\n",
      "Epoch 83/150\n",
      "94/94 [==============================] - 0s 984us/step - loss: 0.0703 - acc: 0.3697\n",
      "Epoch 84/150\n",
      "94/94 [==============================] - 0s 984us/step - loss: 0.0723 - acc: 0.3644\n",
      "Epoch 85/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0702 - acc: 0.3723\n",
      "Epoch 86/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0710 - acc: 0.3630\n",
      "Epoch 87/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0755 - acc: 0.3604\n",
      "Epoch 88/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0689 - acc: 0.3763\n",
      "Epoch 89/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0701 - acc: 0.3697\n",
      "Epoch 90/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0692 - acc: 0.3684\n",
      "Epoch 91/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0651 - acc: 0.3710\n",
      "Epoch 92/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0713 - acc: 0.3617\n",
      "Epoch 93/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0714 - acc: 0.3723\n",
      "Epoch 94/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0671 - acc: 0.3803\n",
      "Epoch 95/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0664 - acc: 0.3670\n",
      "Epoch 96/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0695 - acc: 0.3684\n",
      "Epoch 97/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0704 - acc: 0.3697\n",
      "Epoch 98/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0707 - acc: 0.3630\n",
      "Epoch 99/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0676 - acc: 0.3697\n",
      "Epoch 100/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0660 - acc: 0.3684\n",
      "Epoch 101/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0670 - acc: 0.3657\n",
      "Epoch 102/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0650 - acc: 0.3803\n",
      "Epoch 103/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0690 - acc: 0.3657\n",
      "Epoch 104/150\n",
      "94/94 [==============================] - 0s 994us/step - loss: 0.0648 - acc: 0.3710\n",
      "Epoch 105/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0671 - acc: 0.3684\n",
      "Epoch 106/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0672 - acc: 0.3697\n",
      "Epoch 107/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0642 - acc: 0.3803\n",
      "Epoch 108/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0668 - acc: 0.3723\n",
      "Epoch 109/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0664 - acc: 0.3670\n",
      "Epoch 110/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0680 - acc: 0.3723\n",
      "Epoch 111/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0666 - acc: 0.3697\n",
      "Epoch 112/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0669 - acc: 0.3777\n",
      "Epoch 113/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0645 - acc: 0.3777\n",
      "Epoch 114/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0645 - acc: 0.3670\n",
      "Epoch 115/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0656 - acc: 0.3723\n",
      "Epoch 116/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0669 - acc: 0.3670\n",
      "Epoch 117/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0658 - acc: 0.3803\n",
      "Epoch 118/150\n",
      "94/94 [==============================] - 0s 985us/step - loss: 0.0633 - acc: 0.3843\n",
      "Epoch 119/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0646 - acc: 0.3737\n",
      "Epoch 120/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0652 - acc: 0.3763\n",
      "Epoch 121/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0658 - acc: 0.3710\n",
      "Epoch 122/150\n",
      "94/94 [==============================] - 0s 930us/step - loss: 0.0645 - acc: 0.3750\n",
      "Epoch 123/150\n",
      "94/94 [==============================] - 0s 905us/step - loss: 0.0661 - acc: 0.3697\n",
      "Epoch 124/150\n",
      "94/94 [==============================] - 0s 895us/step - loss: 0.0642 - acc: 0.3816\n",
      "Epoch 125/150\n",
      "94/94 [==============================] - 0s 895us/step - loss: 0.0651 - acc: 0.3670\n",
      "Epoch 126/150\n",
      "94/94 [==============================] - 0s 933us/step - loss: 0.0616 - acc: 0.3910\n",
      "Epoch 127/150\n",
      "94/94 [==============================] - 0s 927us/step - loss: 0.0650 - acc: 0.3777\n",
      "Epoch 128/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0608 - acc: 0.3843\n",
      "Epoch 129/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0638 - acc: 0.3816\n",
      "Epoch 130/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0660 - acc: 0.3856\n",
      "Epoch 131/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0613 - acc: 0.3896\n",
      "Epoch 132/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0605 - acc: 0.3816\n",
      "Epoch 133/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0603 - acc: 0.3910\n",
      "Epoch 134/150\n",
      "94/94 [==============================] - 0s 956us/step - loss: 0.0648 - acc: 0.3723\n",
      "Epoch 135/150\n",
      "94/94 [==============================] - 0s 911us/step - loss: 0.0627 - acc: 0.3816\n",
      "Epoch 136/150\n",
      "94/94 [==============================] - 0s 899us/step - loss: 0.0623 - acc: 0.3816\n",
      "Epoch 137/150\n",
      "94/94 [==============================] - 0s 904us/step - loss: 0.0615 - acc: 0.3830\n",
      "Epoch 138/150\n",
      "94/94 [==============================] - 0s 888us/step - loss: 0.0616 - acc: 0.3803\n",
      "Epoch 139/150\n",
      "94/94 [==============================] - 0s 868us/step - loss: 0.0602 - acc: 0.3923\n",
      "Epoch 140/150\n",
      "94/94 [==============================] - 0s 886us/step - loss: 0.0621 - acc: 0.3697\n",
      "Epoch 141/150\n",
      "94/94 [==============================] - 0s 967us/step - loss: 0.0643 - acc: 0.3830\n",
      "Epoch 142/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0619 - acc: 0.3816\n",
      "Epoch 143/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0623 - acc: 0.3803\n",
      "Epoch 144/150\n",
      "94/94 [==============================] - 0s 972us/step - loss: 0.0618 - acc: 0.3816\n",
      "Epoch 145/150\n",
      "94/94 [==============================] - 0s 975us/step - loss: 0.0581 - acc: 0.3803\n",
      "Epoch 146/150\n",
      "94/94 [==============================] - 0s 872us/step - loss: 0.0613 - acc: 0.3830\n",
      "Epoch 147/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0618 - acc: 0.3843\n",
      "Epoch 148/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0595 - acc: 0.3843\n",
      "Epoch 149/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0625 - acc: 0.3790\n",
      "Epoch 150/150\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.0586 - acc: 0.3883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d3d17f0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[utterance_matrices, people_hint_matrices],\n",
    "          y=[target_matrices],\n",
    "          epochs=150,\n",
    "          sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_matrices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design fake data to force the model to pay attention to the hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mono_discussion_count = len(people)#+1\n",
    "fake_utterance_length = 40\n",
    "fake_mono_utterance_matrices = np.zeros((fake_mono_discussion_count, 1, len(word_index)+2))\n",
    "fake_mono_narration_matrices = np.zeros((fake_mono_discussion_count, 1, len(word_index)+2))\n",
    "\n",
    "fake_mono_people_hint_matrices = np.zeros((fake_mono_discussion_count, 1, len(people)))\n",
    "fake_mono_people_hint_matrices[np.arange(len(people)), 0, np.arange(len(people))] = 1\n",
    "fake_mono_target_matrices = np.zeros((fake_mono_discussion_count, 1, len(people)+1))\n",
    "fake_mono_target_matrices[np.arange(len(people)), 0, np.arange(len(people))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mono_utterance_matrices = fake_mono_utterance_matrices.repeat(50, axis=0)\n",
    "fake_mono_narration_matrices = fake_mono_narration_matrices.repeat(50, axis=0)\n",
    "fake_mono_people_hint_matrices = fake_mono_people_hint_matrices.repeat(50, axis=0)\n",
    "fake_mono_target_matrices = fake_mono_target_matrices.repeat(50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(fake_mono_utterance_matrices.shape[0]):\n",
    "    for j in range(fake_mono_utterance_matrices.shape[1]):\n",
    "        fake_mono_utterance_matrices[i][j][np.random.choice(fake_mono_utterance_matrices.shape[2], fake_utterance_length)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 2s 689us/step - loss: 3.3883 - acc: 0.2356\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 2s 652us/step - loss: 3.3707 - acc: 0.2374\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 2s 736us/step - loss: 3.3606 - acc: 0.2437\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 2s 662us/step - loss: 3.3515 - acc: 0.2556\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 2s 702us/step - loss: 3.3436 - acc: 0.2652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a3e6e80>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=[fake_mono_utterance_matrices, fake_mono_people_hint_matrices],\n",
    "    y=[fake_mono_target_matrices],\n",
    "    verbose=1,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 1s 198us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4065698538886176, 1.0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(fake_mono_target_matrices[[0, 50]])\n",
    "model.evaluate([fake_mono_utterance_matrices, fake_mono_people_hint_matrices], [fake_mono_target_matrices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design fake data to force the model to pay attention to alternative speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_people_pairs = np.array([np.random.choice(len(people), 2) for _ in range(500)])\n",
    "\n",
    "fake_duo_discussion_count = len(fake_people_pairs)\n",
    "fake_duo_discussion_length = 32\n",
    "\n",
    "fake_duo_utterance_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(word_index)+2))\n",
    "fake_duo_narration_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(word_index)+2))\n",
    "\n",
    "fake_duo_people_hint_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(people)))\n",
    "# get a hint in the first utterance about the first speaker\n",
    "fake_duo_people_hint_matrices[np.arange(fake_duo_discussion_count), 0, fake_people_pairs[:, 0]] = 1\n",
    "# get a hint in the second utterance about the second speaker\n",
    "fake_duo_people_hint_matrices[np.arange(fake_duo_discussion_count), 1, fake_people_pairs[:, 1]] = 1\n",
    "\n",
    "for i in range(fake_duo_utterance_matrices.shape[0]):\n",
    "    for j in range(fake_duo_utterance_matrices.shape[1]):\n",
    "        fake_duo_utterance_matrices[i][j][np.random.choice(fake_duo_utterance_matrices.shape[2], fake_utterance_length)] = 1\n",
    "        \n",
    "fake_duo_target_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(people)+1))\n",
    "for i in range(fake_duo_discussion_length//2):\n",
    "    fake_duo_target_matrices[np.arange(fake_duo_discussion_count), i*2, fake_people_pairs[:, 0]] = 1\n",
    "    fake_duo_target_matrices[np.arange(fake_duo_discussion_count), i*2+1, fake_people_pairs[:, 1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 4ms/step\n",
      "[2.1751780014038085, 0.45024999952316286]\n",
      "500/500 [==============================] - 1s 2ms/step\n",
      "[1.1733175048828124, 0.5223750009536743]\n",
      "500/500 [==============================] - 1s 2ms/step\n",
      "[0.8949550933837891, 0.5454375]\n",
      "500/500 [==============================] - 1s 2ms/step\n",
      "[0.7860103082656861, 0.6003750004768371]\n",
      "500/500 [==============================] - 1s 2ms/step\n",
      "[0.44410645246505737, 0.9123749995231628]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.fit(\n",
    "        x=[fake_duo_utterance_matrices, fake_duo_people_hint_matrices],\n",
    "        y=[fake_duo_target_matrices],\n",
    "        verbose=0,\n",
    "        epochs=50\n",
    "    )\n",
    "    print(model.evaluate([fake_duo_utterance_matrices, fake_duo_people_hint_matrices], [fake_duo_target_matrices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.0683e-02, 1.5623e-02, 1.6022e-02, ..., 2.1181e-02, 2.0678e-02, 3.2961e-03],\n",
       "        [2.1510e-03, 9.1679e-03, 5.4895e-03, ..., 1.9287e-02, 2.1664e-02, 4.3625e-04],\n",
       "        [1.7540e-04, 2.3571e-03, 6.7826e-04, ..., 5.5822e-03, 1.1317e-02, 4.1458e-05],\n",
       "        ...,\n",
       "        [1.5415e-06, 3.9914e-04, 8.1925e-06, ..., 2.7226e-04, 5.8026e-03, 5.8685e-06],\n",
       "        [2.0408e-06, 4.4793e-04, 9.4922e-06, ..., 3.1717e-04, 6.1624e-03, 6.8599e-06],\n",
       "        [1.6265e-06, 4.0058e-04, 8.4284e-06, ..., 2.7779e-04, 5.8704e-03, 5.7684e-06]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(fake_duo_target_matrices[0, :2])\n",
    "model.predict([fake_duo_utterance_matrices[:1], fake_duo_people_hint_matrices[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_discussion = discussion_int\n",
    "nb_replique_max = l_max\n",
    "nombre_mots = longueur\n",
    "embedding_incise_dim = nb_personnages+1\n",
    "\n",
    "#EMBEDDING\n",
    "embedding_output_dim = 1\n",
    "\n",
    "input_text = keras.layers.Input(shape =(nb_replique_max, nombre_mots,), name = \"main\")\n",
    "embedding_layer = keras.layers.Embedding(nombre_mots, embedding_output_dim, input_length = None,\n",
    "                                        embeddings_initializer='uniform',embeddings_regularizer=None, \n",
    "                                        activity_regularizer=None,embeddings_constraint=None,\n",
    "                                        mask_zero=False)(input_text)\n",
    "\n",
    "\n",
    "#BAG_OF_WORDS\n",
    "bag = keras.layers.Lambda(lambda x: K.sum(x, axis=2), output_shape=(nb_replique_max, embedding_output_dim,))(embedding_layer)\n",
    "\n",
    "\n",
    "#INPUTBIS\n",
    "input_incise = keras.layers.Input((nb_replique_max, embedding_incise_dim, ), name = \"aux\")\n",
    "\n",
    "\n",
    "#MERGING\n",
    "merged = keras.layers.concatenate([bag, input_incise])\n",
    "\n",
    "\n",
    "#LSTM\n",
    "units = nb_personnages + 1\n",
    "#lstm = keras.layers.LSTM(units, return_sequences=True) (merged)\n",
    "lstm = keras.layers.LSTM(units, return_sequences=True) (input_incise)\n",
    "\n",
    "#MODEL\n",
    "\n",
    "sample = np.array([[int(col != 0) for col in lig] for lig in people])\n",
    "\n",
    "#OUT\n",
    "out = keras.layers.Activation('softmax') (lstm)\n",
    "\n",
    "model = keras.Model(inputs = [input_text, input_incise] , outputs = out)\n",
    "#model = keras.Model(inputs = input_incise, outputs = out)\n",
    "sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', \n",
    "              metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "\n",
    "\n",
    "#data_zero = np.zeros((discussion_int, l_max, longueur))\n",
    "x_train = [data,Z]\n",
    "y_train = Y\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=1,\n",
    "          sample_weight = sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
