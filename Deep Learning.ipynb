{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perceval/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/perceval/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=100, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"UTTERANCE_LENGTH\": 60,\n",
    "    \"DISCUSSION_LENGTH\": 48, #60,\n",
    "    \"NARRATION_LENGTH\": 30,\n",
    "    \"MAX_WORDS_COUNT\": 3000,\n",
    "    \"WORD_EMBEDDING_DIM\": 32,\n",
    "    \"PEOPLE_EMBEDDING_DIM\": 32,\n",
    "    \"VALIDATION_SPLIT\": 0.2,\n",
    "    \"RECURRENT_UNITS_COUNT\": 32,\n",
    "    \"PATIENCE\": 2,\n",
    "    \"EPOCHS\": 8,\n",
    "    \"BATCH_SIZE\": 16, # not large because every sample is 300 words long\n",
    "    \"LR\": 0.001,\n",
    "    \"MSE_LOSS_WEIGHT\": 500,\n",
    "}\n",
    "\n",
    "\n",
    "TIME_STR = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_MODEL_PATH = os.path.join('./output/', \"model-{}.h5\".format(TIME_STR))\n",
    "CHECKPOINT_PATH = os.path.join('./output/', \"model-{}-checkpoint.h5\".format(TIME_STR))\n",
    "GLOVE_PATH = '/Users/perceval/Developpement/Data/glove.6B.100d/glove.6B.100d.txt'\n",
    "DATA_PATH = 'corpus/dataset_dl.pkl'\n",
    "SPLIT_REGEX = r\"[@_\\w]+|['.,!?;]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pickle.load(open(\"corpus/people.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"corpus/dataset-dl.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('My dear Mr_Bennet,  have you heard that Netherfield Park is let at last?',\n",
       " ' said his lady to him one day, ',\n",
       " ['Charlotte', 'Mrs_Bennet'],\n",
       " ['Mr_Bennet'],\n",
       " 'Mrs_Bennet')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and process it (again !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to stem the data, to make the vocabulary denser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentence(stemmer, sentence, people_names):\n",
    "    tokens = re.findall(SPLIT_REGEX, sentence)\n",
    "    return [stemmer.stem(word) if word not in people_names else word for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr_Bennet', 'is', 'with', 'Mrs_Bennet', \"'\", 's', 'daughter']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_sentence(SnowballStemmer('english'), \"Mr_Bennet is with Mrs_Bennet's daughter\", [p['main'] for p in people])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(list, length, default):\n",
    "    \"\"\"Pads the `list`, adding `default` as many times as necessary to reach the provided `length`\"\"\"\n",
    "    if length is None:\n",
    "        return list\n",
    "    return  [list[i] if i < len(list) else default for i in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, people, verbose=2, utterance_length=8, narration_length=None, discussion_length=None):\n",
    "    \"\"\"Tokenizes, stems and transform into to ids the provided `dataset`\n",
    "    \"\"\"\n",
    "    people_main = [p['main'] for p in people]\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    stemmed_samples = [\n",
    "        [(stem_sentence(stemmer, utterance[0], people_main), # utterance part\n",
    "          stem_sentence(stemmer, utterance[1], people_main), # narration part\n",
    "          utterance[2], # potential subjects\n",
    "          utterance[3], # potential destinator\n",
    "          utterance[4]) # label\n",
    "         for utterance in discussion]\n",
    "        for discussion in (tqdm(dataset, desc=\"Text words stemming\")\n",
    "                           if verbose > 1 else dataset)]\n",
    "    \n",
    "    words = [word\n",
    "             for discussion in stemmed_samples\n",
    "             for utterance in discussion\n",
    "             for text in (utterance[0], utterance[1])\n",
    "             for word in text]\n",
    "    # Fit the tokenizer on train texts\n",
    "    word_index, word_counts = np.unique(words, return_counts=True)\n",
    "    new_indices = sorted(range(len(word_index)), key=lambda i: \"0\"+word_index[i] if word_index[i] in people_main else \"1\"+word_index[i])\n",
    "    word_index = word_index[new_indices]\n",
    "    word_counts = word_counts[new_indices]\n",
    "    \n",
    "    inverse_words = {v: i+2 for i, v in enumerate(word_index)}\n",
    "    inverse_people = {v: i for i, v in enumerate(people_main)}\n",
    "\n",
    "    # Convert them to indices and truncate them if they are too large\n",
    "    tokenized_samples = [\n",
    "        # Pad the discussion so that its length matches `discussion_length`\n",
    "        pad([(pad([inverse_words.get(w, 1) for w in utterance[0]], utterance_length, 0),\n",
    "              pad([inverse_words.get(w, 1) for w in utterance[1]], narration_length, 0),\n",
    "              [inverse_people.get(p, 0) for p in utterance[2]],\n",
    "              [inverse_people.get(p, 0) for p in utterance[3]],\n",
    "              inverse_people.get(utterance[4], -1)+1)\n",
    "              for utterance in discussion if utterance[4] in inverse_people],\n",
    "             length=discussion_length,\n",
    "             default=([0]*(utterance_length or 0), # empty utterance\n",
    "                      [0]*(narration_length or 0), # empty narration,\n",
    "                      [], # no target hint\n",
    "                      [], # no target hint\n",
    "                      0, # default non-character id\n",
    "             ))\n",
    "        for discussion in (tqdm(stemmed_samples, desc=\"Text/targets to ids mapping\")\n",
    "                           if verbose > 1 else stemmed_samples)]\n",
    "\n",
    "    return tokenized_samples, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text words stemming: 100%|██████████| 94/94 [00:02<00:00, 39.83it/s]\n",
      "Text/targets to ids mapping: 100%|██████████| 94/94 [00:00<00:00, 683.70it/s]\n"
     ]
    }
   ],
   "source": [
    "res = make_data(dataset, people,\n",
    "                utterance_length=params[\"UTTERANCE_LENGTH\"],\n",
    "                narration_length=params[\"NARRATION_LENGTH\"],\n",
    "                discussion_length=params[\"DISCUSSION_LENGTH\"])\n",
    "processed_dataset, word_index = res\n",
    "#res[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape the dataset as matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrices(data, discussion_length, voc_size, people_count, target_count):\n",
    "    \"\"\"Transform a list of samples into a tuple of matrices to feed into the model\"\"\"\n",
    "    utterance_matrices = np.zeros((len(data), discussion_length, voc_size)) # Set of words -> Bag of words one-hot encoding\n",
    "    narration_matrices = np.zeros((len(data), discussion_length, voc_size)) # Set of words -> Bag of words one-hot encoding\n",
    "    speaker_matrices = np.zeros((len(data), discussion_length, people_count)) # Set of people -> Bag of words one-hot encoding\n",
    "    dest_matrices = np.zeros((len(data), discussion_length, people_count)) # Set of people -> Bag of words one-hot encoding\n",
    "    target_matrices = np.zeros((len(data), discussion_length, target_count)) # Categorical target -> One-hot encoding\n",
    "    for discussion_i, discussion in enumerate(data):\n",
    "        for utterance_i, utterance in enumerate(discussion):\n",
    "            utterance_matrices[discussion_i, utterance_i, utterance[0]] = 1\n",
    "            narration_matrices[discussion_i, utterance_i, utterance[1]] = 1\n",
    "            speaker_matrices[discussion_i, utterance_i, list(utterance[2])] = 1\n",
    "            dest_matrices[discussion_i, utterance_i, list(utterance[3])] = 1\n",
    "            target_matrices[discussion_i, utterance_i, utterance[4]] = 1\n",
    "    \n",
    "    return utterance_matrices, narration_matrices, speaker_matrices, dest_matrices, target_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_matrices, narration_matrices, speaker_hint_matrices, destinator_hint_matrices, target_matrices = \\\n",
    "    make_matrices(processed_dataset,\n",
    "              discussion_length=params['DISCUSSION_LENGTH'],\n",
    "              voc_size=len(word_index)+2,\n",
    "              people_count=len(people),\n",
    "              target_count=len(people)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_input_layer = keras.layers.Input(shape=(None, len(word_index)+2)) # leave the dataset length that will be batched\n",
    "narrations_input_layer = keras.layers.Input(shape=(None, len(word_index)+2)) # leave the dataset length that will be batched\n",
    "speaker_hint_input_layer = keras.layers.Input(shape=(None, len(people))) # leave the dataset length that will be batched\n",
    "dest_hint_input_layer = keras.layers.Input(shape=(None, len(people))) # leave the dataset length that will be batched\n",
    "\n",
    "#embedding_layer = keras.layers.Embedding(len(word_index), params[\"WORD_EMBEDDING_DIM\"], name=\"word_embedding\")\n",
    "word_bag_layer = keras.layers.Dense(params[\"WORD_EMBEDDING_DIM\"])#, kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "people_hint_bag_layer = keras.layers.Dense(params[\"PEOPLE_EMBEDDING_DIM\"])\n",
    "#dest_hint_bag_layer = keras.layers.Dense(params[\"PEOPLE_EMBEDDING_DIM\"])\n",
    "concat_layer = keras.layers.Concatenate(name=\"lstm_input\")\n",
    "\n",
    "concat_input = concat_layer([\n",
    "    word_bag_layer(utterances_input_layer),\n",
    "    word_bag_layer(narrations_input_layer),\n",
    "    people_hint_bag_layer(speaker_hint_input_layer),\n",
    "    people_hint_bag_layer(dest_hint_input_layer),\n",
    "])\n",
    "lstm_input = concat_input#keras.layers.Activation('relu')(concat_input)\n",
    "\n",
    "# not mandatory to set the LSTM dim output to the people hints dim input but it seems more coherent\n",
    "\n",
    "lstm_layer = keras.layers.GRU(params['RECURRENT_UNITS_COUNT'], return_sequences=True)\n",
    "lstm_output = lstm_layer(lstm_input)\n",
    "\n",
    "output_layer = keras.layers.TimeDistributed(keras.layers.Dense(len(people)+1, activation='softmax'))\n",
    "#lstm_layer = keras.layers.Dense(len(people)+1, activation='softmax')\n",
    "output = output_layer(lstm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs = [utterances_input_layer, narrations_input_layer, speaker_hint_input_layer, dest_hint_input_layer] , outputs = [output])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'],\n",
    "              sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weighting\n",
    "\n",
    "Because we have imbalanced classes and padding utterances, we need to weights them to correct the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_utterance_matrices,   test_utterance_matrices,\n",
    " train_narration_matrices,   test_narration_matrices,\n",
    " train_speaker_hint_matrices, test_speaker_hint_matrices,\n",
    " train_dest_hint_matrices, test_dest_hint_matrices,\n",
    " train_target_matrices, test_target_matrices) = \\\n",
    "    train_test_split(utterance_matrices, narration_matrices, speaker_hint_matrices, destinator_hint_matrices, target_matrices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the classes\n",
    "targets_set, targets_inverse, targets_count = np.unique(np.argwhere(train_target_matrices)[:, -1], return_inverse=True, return_counts=True)\n",
    "\n",
    "# And transform these counts into samples\n",
    "sample_weight = np.zeros_like(targets_inverse, dtype=float)\n",
    "sample_weight[targets_inverse == 0] = 0.00\n",
    "total = sum(targets_count) - targets_count[0]\n",
    "for target_id, target_count in zip(targets_set, targets_count):\n",
    "    sample_weight[targets_inverse == target_id] = target_count/total\n",
    "sample_weight = sample_weight.reshape(train_target_matrices.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid zero-sum sample weights\n",
    "#sample_weight[np.argwhere(sample_weight.sum(axis=1) == 0).reshape(-1)] += 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60 samples, validate on 15 samples\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0204 - acc: 0.7444 - val_loss: 0.0629 - val_acc: 0.7208\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0207 - acc: 0.7455 - val_loss: 0.0634 - val_acc: 0.7208\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0203 - acc: 0.7455 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0200 - acc: 0.7462 - val_loss: 0.0636 - val_acc: 0.7208\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0204 - acc: 0.7469 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0201 - acc: 0.7469 - val_loss: 0.0636 - val_acc: 0.7208\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0196 - acc: 0.7472 - val_loss: 0.0637 - val_acc: 0.7208\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0193 - acc: 0.7472 - val_loss: 0.0636 - val_acc: 0.7208\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0197 - acc: 0.7476 - val_loss: 0.0636 - val_acc: 0.7208\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0193 - acc: 0.7483 - val_loss: 0.0636 - val_acc: 0.7208\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0190 - acc: 0.7483 - val_loss: 0.0633 - val_acc: 0.7208\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0196 - acc: 0.7483 - val_loss: 0.0634 - val_acc: 0.7208\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0188 - acc: 0.7486 - val_loss: 0.0636 - val_acc: 0.7208\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0188 - acc: 0.7490 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0185 - acc: 0.7497 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 1s 19ms/step - loss: 0.0183 - acc: 0.7497 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 1s 21ms/step - loss: 0.0187 - acc: 0.7497 - val_loss: 0.0634 - val_acc: 0.7208\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0182 - acc: 0.7500 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 1s 22ms/step - loss: 0.0174 - acc: 0.7500 - val_loss: 0.0635 - val_acc: 0.7208\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 1s 23ms/step - loss: 0.0175 - acc: 0.7500 - val_loss: 0.0635 - val_acc: 0.7194\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0181 - acc: 0.7503 - val_loss: 0.0637 - val_acc: 0.7194\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0172 - acc: 0.7503 - val_loss: 0.0632 - val_acc: 0.7194\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 1s 21ms/step - loss: 0.0170 - acc: 0.7503 - val_loss: 0.0631 - val_acc: 0.7194\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0168 - acc: 0.7503 - val_loss: 0.0632 - val_acc: 0.7194\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 1s 19ms/step - loss: 0.0168 - acc: 0.7507 - val_loss: 0.0637 - val_acc: 0.7194\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0171 - acc: 0.7510 - val_loss: 0.0628 - val_acc: 0.7194\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0166 - acc: 0.7510 - val_loss: 0.0629 - val_acc: 0.7194\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0166 - acc: 0.7514 - val_loss: 0.0633 - val_acc: 0.7194\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0165 - acc: 0.7517 - val_loss: 0.0635 - val_acc: 0.7194\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0163 - acc: 0.7517 - val_loss: 0.0637 - val_acc: 0.7194\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0161 - acc: 0.7517 - val_loss: 0.0639 - val_acc: 0.7194\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0161 - acc: 0.7521 - val_loss: 0.0643 - val_acc: 0.7194\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0156 - acc: 0.7531 - val_loss: 0.0643 - val_acc: 0.7194\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.0154 - acc: 0.7538 - val_loss: 0.0641 - val_acc: 0.7194\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0154 - acc: 0.7556 - val_loss: 0.0643 - val_acc: 0.7194\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0153 - acc: 0.7549 - val_loss: 0.0643 - val_acc: 0.7194\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0152 - acc: 0.7559 - val_loss: 0.0645 - val_acc: 0.7194\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0153 - acc: 0.7563 - val_loss: 0.0645 - val_acc: 0.7194\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0148 - acc: 0.7559 - val_loss: 0.0646 - val_acc: 0.7194\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0147 - acc: 0.7576 - val_loss: 0.0646 - val_acc: 0.7194\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0149 - acc: 0.7580 - val_loss: 0.0644 - val_acc: 0.7194\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0145 - acc: 0.7576 - val_loss: 0.0647 - val_acc: 0.7194\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0146 - acc: 0.7576 - val_loss: 0.0648 - val_acc: 0.7194\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0141 - acc: 0.7576 - val_loss: 0.0647 - val_acc: 0.7194\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0145 - acc: 0.7590 - val_loss: 0.0646 - val_acc: 0.7194\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0138 - acc: 0.7632 - val_loss: 0.0646 - val_acc: 0.7194\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0139 - acc: 0.7653 - val_loss: 0.0645 - val_acc: 0.7194\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0137 - acc: 0.7642 - val_loss: 0.0647 - val_acc: 0.7194\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.0136 - acc: 0.7660 - val_loss: 0.0647 - val_acc: 0.7194\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 1s 19ms/step - loss: 0.0132 - acc: 0.7656 - val_loss: 0.0643 - val_acc: 0.7194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x141d52630>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_utterance_matrices, train_narration_matrices, train_speaker_hint_matrices, train_dest_hint_matrices],\n",
    "          y=[train_target_matrices],\n",
    "          epochs=50,\n",
    "          sample_weight=sample_weight,\n",
    "          validation_split=0.2,\n",
    "          verbose=1, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061611374407582936"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_argmax = test_target_matrices.argmax(axis=2).reshape(-1)\n",
    "accuracy_score(\n",
    "    model.predict(x=[test_utterance_matrices, test_narration_matrices, test_speaker_hint_matrices, test_dest_hint_matrices]).argmax(axis=2).reshape(-1),\n",
    "    test_target_matrices.argmax(axis=2).reshape(-1),\n",
    "    sample_weight=(target_argmax != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11853245531514581"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_argmax = train_target_matrices.argmax(axis=2).reshape(-1)\n",
    "accuracy_score(\n",
    "    model.predict(x=[train_utterance_matrices, train_narration_matrices, train_speaker_hint_matrices, train_dest_hint_matrices]).argmax(axis=2).reshape(-1),\n",
    "    train_target_matrices.argmax(axis=2).reshape(-1),\n",
    "    sample_weight=(target_argmax != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design fake data to force the model to pay attention to the hints\n",
    "\n",
    "We use this data to evaluate the performance of a structure according to properties we know it should have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mono_discussion_count = len(people)#+1\n",
    "fake_utterance_length = 40\n",
    "fake_mono_utterance_matrices = np.zeros((fake_mono_discussion_count, 1, len(word_index)+2))\n",
    "fake_mono_narration_matrices = np.zeros((fake_mono_discussion_count, 1, len(word_index)+2))\n",
    "\n",
    "fake_mono_people_hint_matrices = np.zeros((fake_mono_discussion_count, 1, len(people)))\n",
    "fake_mono_people_hint_matrices[np.arange(len(people)), 0, np.arange(len(people))] = 1\n",
    "fake_mono_target_matrices = np.zeros((fake_mono_discussion_count, 1, len(people)+1))\n",
    "fake_mono_target_matrices[np.arange(len(people)), 0, np.arange(len(people))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mono_utterance_matrices = fake_mono_utterance_matrices.repeat(50, axis=0)\n",
    "fake_mono_narration_matrices = fake_mono_narration_matrices.repeat(50, axis=0)\n",
    "fake_mono_people_hint_matrices = fake_mono_people_hint_matrices.repeat(50, axis=0)\n",
    "fake_mono_target_matrices = fake_mono_target_matrices.repeat(50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(fake_mono_utterance_matrices.shape[0]):\n",
    "    for j in range(fake_mono_utterance_matrices.shape[1]):\n",
    "        fake_mono_utterance_matrices[i][j][np.random.choice(fake_mono_utterance_matrices.shape[2], fake_utterance_length)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 2s 689us/step - loss: 3.3883 - acc: 0.2356\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 2s 652us/step - loss: 3.3707 - acc: 0.2374\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 2s 736us/step - loss: 3.3606 - acc: 0.2437\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 2s 662us/step - loss: 3.3515 - acc: 0.2556\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 2s 702us/step - loss: 3.3436 - acc: 0.2652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a3e6e80>"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=[fake_mono_utterance_matrices, fake_mono_people_hint_matrices],\n",
    "    y=[fake_mono_target_matrices],\n",
    "    verbose=1,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700/2700 [==============================] - 1s 198us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4065698538886176, 1.0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(fake_mono_target_matrices[[0, 50]])\n",
    "model.evaluate([fake_mono_utterance_matrices, fake_mono_people_hint_matrices], [fake_mono_target_matrices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design fake data to force the model to pay attention to alternative speakers\n",
    "\n",
    "We use this data to evaluate the performance of a structure according to properties we know it should have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_people_pairs = np.array([np.random.choice(len(people), 2) for _ in range(500)])\n",
    "\n",
    "fake_duo_discussion_count = len(fake_people_pairs)\n",
    "fake_duo_discussion_length = 32\n",
    "\n",
    "fake_duo_utterance_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(word_index)+2))\n",
    "fake_duo_narration_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(word_index)+2))\n",
    "\n",
    "fake_duo_people_hint_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(people)))\n",
    "# get a hint in the first utterance about the first speaker\n",
    "fake_duo_people_hint_matrices[np.arange(fake_duo_discussion_count), 0, fake_people_pairs[:, 0]] = 1\n",
    "# get a hint in the second utterance about the second speaker\n",
    "fake_duo_people_hint_matrices[np.arange(fake_duo_discussion_count), 1, fake_people_pairs[:, 1]] = 1\n",
    "\n",
    "for i in range(fake_duo_utterance_matrices.shape[0]):\n",
    "    for j in range(fake_duo_utterance_matrices.shape[1]):\n",
    "        fake_duo_utterance_matrices[i][j][np.random.choice(fake_duo_utterance_matrices.shape[2], fake_utterance_length)] = 1\n",
    "        \n",
    "fake_duo_target_matrices = np.zeros((fake_duo_discussion_count, fake_duo_discussion_length, len(people)+1))\n",
    "for i in range(fake_duo_discussion_length//2):\n",
    "    fake_duo_target_matrices[np.arange(fake_duo_discussion_count), i*2, fake_people_pairs[:, 0]] = 1\n",
    "    fake_duo_target_matrices[np.arange(fake_duo_discussion_count), i*2+1, fake_people_pairs[:, 1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(fake_mono_target_matrices[[0, 50]])\n",
    "model.evaluate([fake_duo_utterance_matrices, fake_duo_people_hint_matrices], [fake_duo_target_matrices])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
